This project is an AI-powered assistive tool that uses a YOLOv8s-seg deep learning model to detect and describe objects in real time for visually impaired users. It processes webcam or image input, identifies objects and their relative positions (e.g., left, right, front), and delivers spoken feedback using either Google Cloud Text-to-Speech or offline pyttsx3. The system includes a web-based React interface connected to a Flask backend running an ONNX model, and also offers a lightweight desktop version built with Tkinter for direct use.
