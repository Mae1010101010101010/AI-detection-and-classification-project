{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e72240b9-fd45-4dd1-88a6-80569738cffd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:28:49,538 - INFO - ADE20K raw data directory set to: /home/sagemaker-user/user-default-efs/ADE20K_2021_17_01\n",
      "2025-05-13 01:28:49,539 - INFO - Index file path set to: /home/sagemaker-user/user-default-efs/ADE20K_2021_17_01/index_ade20k.pkl\n",
      "2025-05-13 01:28:49,539 - INFO - YOLOv8 dataset root directory set to: /home/sagemaker-user/ADE20K_YOLOv8_Dataset\n",
      "2025-05-13 01:28:49,541 - INFO - Random state seed set to: 42028\n",
      "2025-05-13 01:28:49,541 - INFO - Data split ratios: Train=0.8, Validation=0.1, Test=0.1\n",
      "2025-05-13 01:28:49,542 - INFO - YOLO dataset root directory already exists: /home/sagemaker-user/ADE20K_YOLOv8_Dataset\n",
      "2025-05-13 01:28:49,543 - INFO - Ensured creation of train, val, and test subdirectories for images and labels.\n",
      "2025-05-13 01:28:49,544 - INFO - Cell 1 execution completed: Initial setup and configuration are finished.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Initial Setup and Configuration\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "# Configure basic logging for feedback during execution.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Configuration Constants ---\n",
    "\n",
    "# Defines the root directory where the ADE20K dataset is stored.\n",
    "# Assumes the notebook is in the root folder, and 'user-default-efs' is a subdirectory.\n",
    "ADE20K_RAW_DATA_DIR = os.path.join('.', 'user-default-efs', 'ADE20K_2021_17_01')\n",
    "logging.info(f\"ADE20K raw data directory set to: {os.path.abspath(ADE20K_RAW_DATA_DIR)}\")\n",
    "\n",
    "# Specifies the path to the index file containing dataset metadata.\n",
    "INDEX_FILE_PATH = os.path.join(ADE20K_RAW_DATA_DIR, 'index_ade20k.pkl')\n",
    "logging.info(f\"Index file path set to: {os.path.abspath(INDEX_FILE_PATH)}\")\n",
    "\n",
    "# Defines the root directory where the processed YOLOv8 compatible dataset will be stored.\n",
    "YOLO_DATASET_ROOT_DIR = os.path.join('.', 'ADE20K_YOLOv8_Dataset')\n",
    "logging.info(f\"YOLOv8 dataset root directory set to: {os.path.abspath(YOLO_DATASET_ROOT_DIR)}\")\n",
    "\n",
    "# Defines subdirectories for images and labels within the YOLO dataset structure.\n",
    "YOLO_IMAGES_DIR = os.path.join(YOLO_DATASET_ROOT_DIR, 'images')\n",
    "YOLO_LABELS_DIR = os.path.join(YOLO_DATASET_ROOT_DIR, 'labels')\n",
    "\n",
    "# Specifies the seed for random number generators to ensure reproducibility of dataset splits.\n",
    "RANDOM_STATE_SEED = 42028\n",
    "logging.info(f\"Random state seed set to: {RANDOM_STATE_SEED}\")\n",
    "\n",
    "# Specifies the desired splits for training, validation, and testing sets.\n",
    "# The test set size is explicitly defined, and the validation set is a proportion of the remaining data.\n",
    "TRAIN_RATIO = 0.80  # 80% of the data for training.\n",
    "VALIDATION_RATIO = 0.10  # 10% of the data for validation.\n",
    "TEST_RATIO = 0.10  # 10% of the data for testing.\n",
    "# Ensure ratios sum to 1.0 for clarity, though only two are strictly needed to define the three sets.\n",
    "if not np.isclose(TRAIN_RATIO + VALIDATION_RATIO + TEST_RATIO, 1.0):\n",
    "    raise ValueError(\"Train, validation, and test ratios must sum to 1.0.\")\n",
    "logging.info(f\"Data split ratios: Train={TRAIN_RATIO}, Validation={VALIDATION_RATIO}, Test={TEST_RATIO}\")\n",
    "\n",
    "# Create the root directory for the YOLOv8 dataset if it does not already exist.\n",
    "# This helps in organizing the processed data.\n",
    "if not os.path.exists(YOLO_DATASET_ROOT_DIR):\n",
    "    os.makedirs(YOLO_DATASET_ROOT_DIR)\n",
    "    logging.info(f\"Created YOLO dataset root directory: {os.path.abspath(YOLO_DATASET_ROOT_DIR)}\")\n",
    "else:\n",
    "    logging.info(f\"YOLO dataset root directory already exists: {os.path.abspath(YOLO_DATASET_ROOT_DIR)}\")\n",
    "\n",
    "# Create subdirectories for images and labels if they do not exist.\n",
    "# These directories will store the actual image files and their corresponding annotation files.\n",
    "for subset in ['train', 'val', 'test']:\n",
    "    os.makedirs(os.path.join(YOLO_IMAGES_DIR, subset), exist_ok=True)\n",
    "    os.makedirs(os.path.join(YOLO_LABELS_DIR, subset), exist_ok=True)\n",
    "logging.info(\"Ensured creation of train, val, and test subdirectories for images and labels.\")\n",
    "\n",
    "logging.info(\"Cell 1 execution completed: Initial setup and configuration are finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79be9dd-2928-4f1a-add5-04e0950a98f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = './YOLOv8_Training_Runs_ADE20K'\n",
    "RUN_NAME = 'ade20k_yolov8s_seg_run12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61506982-4c1e-4657-aa3f-7a8b73e86883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:28:59,441 - INFO - Defined 85 initial curated object class candidates.\n",
      "2025-05-13 01:28:59,442 - INFO - Attempting to load pickle file from: ./user-default-efs/ADE20K_2021_17_01/index_ade20k.pkl\n",
      "2025-05-13 01:28:59,616 - INFO - Successfully loaded pickle file.\n",
      "2025-05-13 01:28:59,618 - INFO - Performing sanity check for curated class names against dataset object names...\n",
      "2025-05-13 01:28:59,618 - INFO - Matched candidate 'backpack' to dataset name 'backpack, back pack, knapsack, packsack, rucksack, haversack'.\n",
      "2025-05-13 01:28:59,620 - INFO - Matched candidate 'bicycle' to dataset name 'bicycle, bike, wheel, cycle'.\n",
      "2025-05-13 01:28:59,621 - INFO - Matched candidate 'bus' to dataset name 'bus, autobus, coach, charabanc, double-decker, jitney, motorbus, motorcoach, omnibus, passenger vehicle'.\n",
      "2025-05-13 01:28:59,623 - INFO - Matched candidate 'truck' to dataset name 'truck, motortruck'.\n",
      "2025-05-13 01:28:59,623 - INFO - Matched candidate 'airplane' to dataset name 'airplane, aeroplane, plane'.\n",
      "2025-05-13 01:28:59,624 - INFO - Matched candidate 'traffic light' to dataset name 'traffic light, traffic signal, stoplight'.\n",
      "2025-05-13 01:28:59,627 - INFO - Matched candidate 'dog' to dataset name 'dog, domestic dog, Canis familiaris'.\n",
      "2025-05-13 01:28:59,627 - INFO - Matched candidate 'horse' to dataset name 'horse, Equus caballus'.\n",
      "2025-05-13 01:28:59,628 - INFO - Matched candidate 'cow' to dataset name 'cow, moo-cow'.\n",
      "2025-05-13 01:28:59,629 - INFO - Matched candidate 'sofa' to dataset name 'sofa, couch, lounge'.\n",
      "2025-05-13 01:28:59,631 - INFO - Matched candidate 'toilet' to dataset name 'toilet, can, commode, crapper, pot, potty, stool, throne'.\n",
      "2025-05-13 01:28:59,634 - INFO - Matched candidate 'laptop' to dataset name 'laptop, laptop computer'.\n",
      "2025-05-13 01:28:59,636 - INFO - Matched candidate 'remote control' to dataset name 'remote control, remote'.\n",
      "2025-05-13 01:28:59,637 - INFO - Matched candidate 'refrigerator' to dataset name 'refrigerator, icebox'.\n",
      "2025-05-13 01:28:59,641 - INFO - Matched candidate 'stairs' to dataset name 'stairs, steps'.\n",
      "2025-05-13 01:28:59,642 - INFO - Matched candidate 'signboard' to dataset name 'signboard, sign'.\n",
      "2025-05-13 01:28:59,642 - WARNING - The following 13 curated class candidates were NOT directly found or matched as primary part in dataset's objectnames and will be excluded:\n",
      "2025-05-13 01:28:59,643 - WARNING -  - suitcase\n",
      "2025-05-13 01:28:59,644 - WARNING -  - motorcycle\n",
      "2025-05-13 01:28:59,644 - WARNING -  - stop sign\n",
      "2025-05-13 01:28:59,645 - WARNING -  - fire hydrant\n",
      "2025-05-13 01:28:59,645 - WARNING -  - potted plant\n",
      "2025-05-13 01:28:59,646 - WARNING -  - dining table\n",
      "2025-05-13 01:28:59,646 - WARNING -  - bookshelf\n",
      "2025-05-13 01:28:59,647 - WARNING -  - television\n",
      "2025-05-13 01:28:59,647 - WARNING -  - tv\n",
      "2025-05-13 01:28:59,648 - WARNING -  - cell phone\n",
      "2025-05-13 01:28:59,650 - WARNING -  - hair drier\n",
      "2025-05-13 01:28:59,650 - WARNING -  - baseball bat\n",
      "2025-05-13 01:28:59,652 - WARNING -  - sports ball\n",
      "2025-05-13 01:28:59,653 - INFO - Total of 72 curated classes verified and selected for the model:\n",
      "2025-05-13 01:28:59,653 - INFO - 1. airplane, aeroplane, plane (Original ADE20K Index: 13)\n",
      "2025-05-13 01:28:59,654 - INFO - 2. backpack, back pack, knapsack, packsack, rucksack, haversack (Original ADE20K Index: 89)\n",
      "2025-05-13 01:28:59,654 - INFO - 3. bed (Original ADE20K Index: 164)\n",
      "2025-05-13 01:28:59,655 - INFO - 4. bench (Original ADE20K Index: 180)\n",
      "2025-05-13 01:28:59,656 - INFO - 5. bicycle, bike, wheel, cycle (Original ADE20K Index: 186)\n",
      "2025-05-13 01:28:59,656 - INFO - 6. bird (Original ADE20K Index: 197)\n",
      "2025-05-13 01:28:59,657 - INFO - 7. blender (Original ADE20K Index: 209)\n",
      "2025-05-13 01:28:59,657 - INFO - 8. boat (Original ADE20K Index: 222)\n",
      "2025-05-13 01:28:59,659 - INFO - 9. book (Original ADE20K Index: 235)\n",
      "2025-05-13 01:28:59,660 - INFO - 10. bottle (Original ADE20K Index: 248)\n",
      "2025-05-13 01:28:59,660 - INFO - 11. bowl (Original ADE20K Index: 258)\n",
      "2025-05-13 01:28:59,661 - INFO - 12. building (Original ADE20K Index: 3153)\n",
      "2025-05-13 01:28:59,661 - INFO - 13. bus, autobus, coach, charabanc, double-decker, jitney, motorbus, motorcoach, omnibus, passenger vehicle (Original ADE20K Index: 326)\n",
      "2025-05-13 01:28:59,662 - INFO - 14. cabinet (Original ADE20K Index: 349)\n",
      "2025-05-13 01:28:59,662 - INFO - 15. car (Original ADE20K Index: 3136)\n",
      "2025-05-13 01:28:59,663 - INFO - 16. cat (Original ADE20K Index: 429)\n",
      "2025-05-13 01:28:59,663 - INFO - 17. ceiling (Original ADE20K Index: 446)\n",
      "2025-05-13 01:28:59,664 - INFO - 18. chair (Original ADE20K Index: 470)\n",
      "2025-05-13 01:28:59,664 - INFO - 19. clock (Original ADE20K Index: 529)\n",
      "2025-05-13 01:28:59,665 - INFO - 20. cow, moo-cow (Original ADE20K Index: 637)\n",
      "2025-05-13 01:28:59,665 - INFO - 21. cup (Original ADE20K Index: 676)\n",
      "2025-05-13 01:28:59,666 - INFO - 22. cushion (Original ADE20K Index: 688)\n",
      "2025-05-13 01:28:59,666 - INFO - 23. desk (Original ADE20K Index: 723)\n",
      "2025-05-13 01:28:59,667 - INFO - 24. dog, domestic dog, Canis familiaris (Original ADE20K Index: 764)\n",
      "2025-05-13 01:28:59,667 - INFO - 25. door (Original ADE20K Index: 773)\n",
      "2025-05-13 01:28:59,668 - INFO - 26. fence (Original ADE20K Index: 903)\n",
      "2025-05-13 01:28:59,668 - INFO - 27. floor (Original ADE20K Index: 971)\n",
      "2025-05-13 01:28:59,669 - INFO - 28. fork (Original ADE20K Index: 1018)\n",
      "2025-05-13 01:28:59,669 - INFO - 29. handbag (Original ADE20K Index: 1177)\n",
      "2025-05-13 01:28:59,670 - INFO - 30. horse, Equus caballus (Original ADE20K Index: 1265)\n",
      "2025-05-13 01:28:59,670 - INFO - 31. house (Original ADE20K Index: 1275)\n",
      "2025-05-13 01:28:59,671 - INFO - 32. keyboard (Original ADE20K Index: 1369)\n",
      "2025-05-13 01:28:59,672 - INFO - 33. knife (Original ADE20K Index: 1377)\n",
      "2025-05-13 01:28:59,672 - INFO - 34. laptop, laptop computer (Original ADE20K Index: 1406)\n",
      "2025-05-13 01:28:59,673 - INFO - 35. light (Original ADE20K Index: 1442)\n",
      "2025-05-13 01:28:59,673 - INFO - 36. light source (Original ADE20K Index: 1447)\n",
      "2025-05-13 01:28:59,674 - INFO - 37. microwave (Original ADE20K Index: 3135)\n",
      "2025-05-13 01:28:59,674 - INFO - 38. monitor (Original ADE20K Index: 1581)\n",
      "2025-05-13 01:28:59,675 - INFO - 39. mouse (Original ADE20K Index: 1610)\n",
      "2025-05-13 01:28:59,675 - INFO - 40. oven (Original ADE20K Index: 1707)\n",
      "2025-05-13 01:28:59,676 - INFO - 41. parking meter (Original ADE20K Index: 1778)\n",
      "2025-05-13 01:28:59,676 - INFO - 42. person (Original ADE20K Index: 1828)\n",
      "2025-05-13 01:28:59,677 - INFO - 43. pillow (Original ADE20K Index: 1868)\n",
      "2025-05-13 01:28:59,678 - INFO - 44. plate (Original ADE20K Index: 1918)\n",
      "2025-05-13 01:28:59,678 - INFO - 45. pole (Original ADE20K Index: 1935)\n",
      "2025-05-13 01:28:59,678 - INFO - 46. refrigerator, icebox (Original ADE20K Index: 2095)\n",
      "2025-05-13 01:28:59,679 - INFO - 47. remote control, remote (Original ADE20K Index: 2098)\n",
      "2025-05-13 01:28:59,680 - INFO - 48. road (Original ADE20K Index: 2129)\n",
      "2025-05-13 01:28:59,681 - INFO - 49. scissors (Original ADE20K Index: 2241)\n",
      "2025-05-13 01:28:59,682 - INFO - 50. sheep (Original ADE20K Index: 2323)\n",
      "2025-05-13 01:28:59,683 - INFO - 51. sidewalk (Original ADE20K Index: 2375)\n",
      "2025-05-13 01:28:59,686 - INFO - 52. signboard, sign (Original ADE20K Index: 2379)\n",
      "2025-05-13 01:28:59,686 - INFO - 53. sink (Original ADE20K Index: 2387)\n",
      "2025-05-13 01:28:59,687 - INFO - 54. skateboard (Original ADE20K Index: 2393)\n",
      "2025-05-13 01:28:59,688 - INFO - 55. sky (Original ADE20K Index: 2419)\n",
      "2025-05-13 01:28:59,689 - INFO - 56. sofa, couch, lounge (Original ADE20K Index: 2472)\n",
      "2025-05-13 01:28:59,689 - INFO - 57. spoon (Original ADE20K Index: 2504)\n",
      "2025-05-13 01:28:59,690 - INFO - 58. stairs, steps (Original ADE20K Index: 2529)\n",
      "2025-05-13 01:28:59,690 - INFO - 59. street sign (Original ADE20K Index: 2612)\n",
      "2025-05-13 01:28:59,691 - INFO - 60. table (Original ADE20K Index: 2683)\n",
      "2025-05-13 01:28:59,692 - INFO - 61. teddy bear (Original ADE20K Index: 3430)\n",
      "2025-05-13 01:28:59,693 - INFO - 62. tennis racket (Original ADE20K Index: 2736)\n",
      "2025-05-13 01:28:59,693 - INFO - 63. toaster (Original ADE20K Index: 2781)\n",
      "2025-05-13 01:28:59,694 - INFO - 64. toilet, can, commode, crapper, pot, potty, stool, throne (Original ADE20K Index: 2792)\n",
      "2025-05-13 01:28:59,694 - INFO - 65. toothbrush (Original ADE20K Index: 2810)\n",
      "2025-05-13 01:28:59,695 - INFO - 66. traffic light, traffic signal, stoplight (Original ADE20K Index: 2835)\n",
      "2025-05-13 01:28:59,695 - INFO - 67. tree (Original ADE20K Index: 2854)\n",
      "2025-05-13 01:28:59,697 - INFO - 68. truck, motortruck (Original ADE20K Index: 2879)\n",
      "2025-05-13 01:28:59,697 - INFO - 69. umbrella (Original ADE20K Index: 2900)\n",
      "2025-05-13 01:28:59,698 - INFO - 70. vase (Original ADE20K Index: 2931)\n",
      "2025-05-13 01:28:59,699 - INFO - 71. wall (Original ADE20K Index: 2977)\n",
      "2025-05-13 01:28:59,699 - INFO - 72. window (Original ADE20K Index: 3049)\n",
      "2025-05-13 01:28:59,700 - INFO - Created YOLO ID mappings for 72 classes.\n",
      "2025-05-13 01:28:59,700 - INFO - Stored 'objectPresence' array for potential use in data filtering.\n",
      "2025-05-13 01:28:59,701 - INFO - Cell 2 execution completed: Metadata loaded, curated classes defined, verified, and mappings created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Metadata and Define Curated Target Classes\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "# --- Define the Curated List of Target Object Classes ---\n",
    "# This list contains common objects deemed relevant for the application.\n",
    "# It is important to verify these names against the dataset's actual object names.\n",
    "# Some names in ADE20K can be compound (e.g., \"foo, bar, baz\").\n",
    "# The names below are chosen for their general applicability.\n",
    "\n",
    "CURATED_OBJECT_CLASS_CANDIDATES = [\n",
    "    # People & Accessories\n",
    "    'person', 'backpack', 'handbag', 'suitcase', 'umbrella',\n",
    "    # Vehicles\n",
    "    'bicycle', 'car', 'motorcycle', 'bus', 'truck', 'boat', 'airplane',\n",
    "    # Traffic & Street Furniture\n",
    "    'traffic light', 'stop sign', 'fire hydrant', 'parking meter', 'bench', 'street sign',\n",
    "    # Animals (Common urban/domestic)\n",
    "    'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', # Adjusted based on common dataset animals\n",
    "    # Indoor Furniture\n",
    "    'chair', 'sofa', 'potted plant', 'bed', 'dining table', 'table', 'desk', 'toilet', 'door', 'window',\n",
    "    'bookshelf', 'cabinet',\n",
    "    # Electronics\n",
    "    'television', 'tv', 'monitor', 'laptop', 'mouse', 'remote control', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',\n",
    "    'sink', 'refrigerator', 'blender',\n",
    "    # Common Objects\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'bottle', 'cup', 'fork', 'knife',\n",
    "    'spoon', 'bowl', 'plate',\n",
    "    # Sports Equipment (Common)\n",
    "    'tennis racket', 'baseball bat', 'sports ball', 'skateboard',\n",
    "    # Environmental / Structural (Important for scene context)\n",
    "    'building', 'house', 'sky', 'tree', 'road', 'sidewalk', 'wall', 'floor', 'ceiling', 'stairs', 'pole', 'fence', 'signboard',\n",
    "    'light', 'light source', 'cushion', 'pillow'\n",
    "    # Total: 79 candidates. This list can be trimmed or expanded.\n",
    "]\n",
    "logging.info(f\"Defined {len(CURATED_OBJECT_CLASS_CANDIDATES)} initial curated object class candidates.\")\n",
    "\n",
    "# --- Function to Load Pickle File (from previous cell, ensure it's defined or redefine) ---\n",
    "def load_pickle_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads data from a specified pickle file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The full path to the pickle file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The data loaded from the pickle file, or None if an error occurs.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Attempting to load pickle file from: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        logging.error(f\"Pickle file not found at {file_path}.\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        logging.info(\"Successfully loaded pickle file.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading pickle file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Load ADE20K Index Data ---\n",
    "# The INDEX_FILE_PATH variable is expected to be defined in Cell 1.\n",
    "index_data = load_pickle_data(INDEX_FILE_PATH)\n",
    "\n",
    "# Initialize global variables for class lists and mappings\n",
    "TARGET_OBJECT_CLASSES = []\n",
    "CLASS_NAME_TO_YOLO_ID = {}\n",
    "YOLO_ID_TO_CLASS_NAME = {}\n",
    "ADE20K_CLASS_INDICES_FOR_TARGETS = {} # Stores original ADE20K index for our target classes\n",
    "\n",
    "if index_data and 'objectnames' in index_data:\n",
    "    # Convert dataset object names to a set for efficient lookup.\n",
    "    # ADE20K names can be compound (e.g., \"foo, bar, baz\"). A direct match is attempted here.\n",
    "    # For more complex matching, one might need to split dataset names by comma and check parts.\n",
    "    dataset_object_names_set = set(index_data['objectnames'])\n",
    "    dataset_object_names_list = list(index_data['objectnames']) # To get original indices\n",
    "\n",
    "    logging.info(\"Performing sanity check for curated class names against dataset object names...\")\n",
    "    verified_target_classes = []\n",
    "    missing_classes = []\n",
    "\n",
    "    for class_candidate in CURATED_OBJECT_CLASS_CANDIDATES:\n",
    "        # Attempt a direct match first.\n",
    "        # ADE20K names can be specific, e.g., \"television, tv set\".\n",
    "        # This check is case-sensitive.\n",
    "        if class_candidate in dataset_object_names_set:\n",
    "            verified_target_classes.append(class_candidate)\n",
    "            try:\n",
    "                # Store the original ADE20K index of this class name\n",
    "                ade20k_original_idx = dataset_object_names_list.index(class_candidate)\n",
    "                ADE20K_CLASS_INDICES_FOR_TARGETS[class_candidate] = ade20k_original_idx\n",
    "            except ValueError:\n",
    "                # Should not happen if found in set, but as a safeguard\n",
    "                logging.warning(f\"Class '{class_candidate}' found in set but not in list (unexpected).\")\n",
    "        else:\n",
    "            # If direct match fails, try a more lenient check (e.g., if candidate is a substring)\n",
    "            # This is a simple example; more sophisticated matching might be needed if many classes are missed.\n",
    "            found_variant = False\n",
    "            for dataset_name in dataset_object_names_list:\n",
    "                # Check if candidate is a primary part of a compound ADE20K name\n",
    "                # e.g., if candidate is \"television\" and dataset_name is \"television, tv set\"\n",
    "                if class_candidate == dataset_name.split(',')[0].strip():\n",
    "                    verified_target_classes.append(dataset_name) # Use the full dataset name\n",
    "                    try:\n",
    "                        ade20k_original_idx = dataset_object_names_list.index(dataset_name)\n",
    "                        ADE20K_CLASS_INDICES_FOR_TARGETS[dataset_name] = ade20k_original_idx\n",
    "                        logging.info(f\"Matched candidate '{class_candidate}' to dataset name '{dataset_name}'.\")\n",
    "                        found_variant = True\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                         logging.warning(f\"Variant '{dataset_name}' for '{class_candidate}' found but not in list (unexpected).\")\n",
    "            if not found_variant:\n",
    "                missing_classes.append(class_candidate)\n",
    "\n",
    "    TARGET_OBJECT_CLASSES = sorted(list(set(verified_target_classes))) # Ensure uniqueness and sort\n",
    "\n",
    "    if missing_classes:\n",
    "        logging.warning(f\"The following {len(missing_classes)} curated class candidates were NOT directly found or matched as primary part in dataset's objectnames and will be excluded:\")\n",
    "        for mc in missing_classes:\n",
    "            logging.warning(f\" - {mc}\")\n",
    "    \n",
    "    logging.info(f\"Total of {len(TARGET_OBJECT_CLASSES)} curated classes verified and selected for the model:\")\n",
    "    for i, class_name in enumerate(TARGET_OBJECT_CLASSES):\n",
    "        logging.info(f\"{i+1}. {class_name} (Original ADE20K Index: {ADE20K_CLASS_INDICES_FOR_TARGETS.get(class_name, 'N/A')})\")\n",
    "\n",
    "    # --- Create Class Mappings for YOLO ---\n",
    "    # These mappings convert the *verified* class names to integer IDs (0 to N-1) for YOLO training.\n",
    "    CLASS_NAME_TO_YOLO_ID = {name: i for i, name in enumerate(TARGET_OBJECT_CLASSES)}\n",
    "    YOLO_ID_TO_CLASS_NAME = {i: name for i, name in enumerate(TARGET_OBJECT_CLASSES)}\n",
    "\n",
    "    logging.info(f\"Created YOLO ID mappings for {len(TARGET_OBJECT_CLASSES)} classes.\")\n",
    "    # logging.debug(f\"CLASS_NAME_TO_YOLO_ID: {CLASS_NAME_TO_YOLO_ID}\") # For detailed verification\n",
    "\n",
    "    # Store objectPresence for later use if available and if indices align\n",
    "    if 'objectPresence' in index_data:\n",
    "        # This assumes objectPresence rows map to original ADE20K class indices\n",
    "        logging.info(\"Stored 'objectPresence' array for potential use in data filtering.\")\n",
    "    else:\n",
    "        logging.warning(\"'objectPresence' not found in index_data.\")\n",
    "\n",
    "else:\n",
    "    logging.error(\"Failed to load index_data or 'objectnames' key is missing. Cannot proceed with class selection.\")\n",
    "    # Define placeholders to prevent errors in subsequent cells if index_data is None or malformed.\n",
    "    index_data = {'filename': [], 'folder': [], 'scene': [], 'objectnames': [], 'objectcounts': np.array([])}\n",
    "    # No classes means these will be empty, and subsequent cells should handle this.\n",
    "\n",
    "logging.info(\"Cell 2 execution completed: Metadata loaded, curated classes defined, verified, and mappings created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c72f01f-aada-44c5-9f70-c1da374a2225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 11:03:14,825 - INFO - Starting Cell 3: Data Filtering and Splitting.\n",
      "2025-05-12 11:03:14,829 - INFO - Constructing initial manifest for 27574 images based on metadata.\n",
      "2025-05-12 11:03:14,958 - INFO - Successfully constructed initial manifest with 27574 image entries.\n",
      "2025-05-12 11:03:14,959 - INFO - Filtering images based on presence of 72 target classes using the 'objectPresence' matrix (Shape: (3688, 27574)).\n",
      "2025-05-12 11:03:15,354 - INFO - Filtered down to 26692 images containing at least one target class.\n",
      "2025-05-12 11:03:15,376 - INFO - Attempting to split 26692 filtered images into train/validation/test sets.\n",
      "2025-05-12 11:03:15,384 - INFO - Attempting stratified split by 'scene'. Unique scenes: 1642. Scenes suitable for stratification: 973.\n",
      "2025-05-12 11:03:15,405 - WARNING - Stratification for train/temp_val_test split failed: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.. Retrying with random split.\n",
      "2025-05-12 11:03:15,416 - WARNING - Stratification for val/test split failed: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.. Retrying with random split.\n",
      "2025-05-12 11:03:15,418 - INFO - Data splitting process completed.\n",
      "2025-05-12 11:03:15,419 - INFO -   Total images after filtering: 26692\n",
      "2025-05-12 11:03:15,419 - INFO -   Training set size: 21353\n",
      "2025-05-12 11:03:15,420 - INFO -   Validation set size: 2669\n",
      "2025-05-12 11:03:15,421 - INFO -   Test set size: 2670\n",
      "2025-05-12 11:03:15,421 - INFO - Cell 3 execution completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Filtering and Splitting\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# This cell relies on variables defined in previous cells:\n",
    "# index_data, TARGET_OBJECT_CLASSES, ADE20K_CLASS_INDICES_FOR_TARGETS (from Cell 2)\n",
    "# ADE20K_RAW_DATA_DIR, TRAIN_RATIO, VALIDATION_RATIO, TEST_RATIO, RANDOM_STATE_SEED (from Cell 1)\n",
    "\n",
    "logging.info(\"Starting Cell 3: Data Filtering and Splitting.\")\n",
    "\n",
    "# Initialize placeholders for dataframes in case of early exit.\n",
    "train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "filtered_image_data = [] # This will hold the data for images that pass the filter.\n",
    "\n",
    "# Ensure essential data structures from previous cells are available and valid.\n",
    "if not ('index_data' in globals() and index_data and\n",
    "        'filename' in index_data and 'folder' in index_data and 'scene' in index_data and\n",
    "        'TARGET_OBJECT_CLASSES' in globals() and TARGET_OBJECT_CLASSES):\n",
    "    logging.error(\"Essential data from previous cells (index_data, TARGET_OBJECT_CLASSES) \"\n",
    "                  \"is missing or invalid. Cannot proceed with data filtering and splitting.\")\n",
    "else:\n",
    "    all_image_filenames = index_data['filename']\n",
    "    all_folder_paths_from_pickle = index_data['folder'] # Paths as stored in the metadata file.\n",
    "    all_scene_labels = index_data['scene']\n",
    "\n",
    "    initial_manifest = []\n",
    "    logging.info(f\"Constructing initial manifest for {len(all_image_filenames)} images based on metadata.\")\n",
    "\n",
    "    # Normalize ADE20K_RAW_DATA_DIR and extract its basename.\n",
    "    # This helps in intelligently joining paths, especially if metadata paths might contain\n",
    "    # overlapping segments with the base directory path.\n",
    "    normalized_ade20k_raw_dir = os.path.normpath(ADE20K_RAW_DATA_DIR)\n",
    "    dataset_directory_basename = os.path.basename(normalized_ade20k_raw_dir)\n",
    "\n",
    "    for i in range(len(all_image_filenames)):\n",
    "        img_filename = all_image_filenames[i]\n",
    "        folder_path_original = all_folder_paths_from_pickle[i]\n",
    "\n",
    "        # Determine the correct relative path component from the metadata.\n",
    "        # If the path from metadata starts with the dataset's root folder name,\n",
    "        # that part is stripped to avoid duplication when joining with ADE20K_RAW_DATA_DIR.\n",
    "        # This ensures that 'relative_path_to_join' is truly relative to ADE20K_RAW_DATA_DIR.\n",
    "        relative_path_to_join = folder_path_original\n",
    "        if folder_path_original.startswith(dataset_directory_basename + os.path.sep):\n",
    "            relative_path_to_join = folder_path_original[len(dataset_directory_basename) + len(os.path.sep):]\n",
    "        \n",
    "        # Construct the full absolute paths for the image and its corresponding JSON annotation file.\n",
    "        img_path = os.path.join(ADE20K_RAW_DATA_DIR, relative_path_to_join, img_filename)\n",
    "        \n",
    "        base_filename = os.path.splitext(img_filename)[0]\n",
    "        json_filename = base_filename + \".json\"\n",
    "        json_path = os.path.join(ADE20K_RAW_DATA_DIR, relative_path_to_join, json_filename)\n",
    "\n",
    "        initial_manifest.append({\n",
    "            'image_path': img_path,\n",
    "            'json_path': json_path,\n",
    "            'scene': all_scene_labels[i],\n",
    "            'image_idx_original': i # Preserves original index for 'objectPresence' lookup.\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Successfully constructed initial manifest with {len(initial_manifest)} image entries.\")\n",
    "\n",
    "    # --- Filter Manifest for Target Classes using 'objectPresence' ---\n",
    "    if 'objectPresence' in index_data and hasattr(index_data['objectPresence'], 'shape'):\n",
    "        object_presence_matrix = index_data['objectPresence']\n",
    "        \n",
    "        # Collect original ADE20K indices for the successfully verified target classes.\n",
    "        target_class_original_indices = []\n",
    "        if 'ADE20K_CLASS_INDICES_FOR_TARGETS' in globals():\n",
    "             target_class_original_indices = [ADE20K_CLASS_INDICES_FOR_TARGETS[name] \n",
    "                                             for name in TARGET_OBJECT_CLASSES \n",
    "                                             if name in ADE20K_CLASS_INDICES_FOR_TARGETS]\n",
    "\n",
    "        if not target_class_original_indices:\n",
    "            logging.warning(\"No original ADE20K indices found for target classes. \"\n",
    "                            \"Filtering based on 'objectPresence' will likely result in an empty dataset.\")\n",
    "        else:\n",
    "            logging.info(f\"Filtering images based on presence of {len(target_class_original_indices)} target classes \"\n",
    "                         f\"using the 'objectPresence' matrix (Shape: {object_presence_matrix.shape}).\")\n",
    "            \n",
    "            for item_data in initial_manifest:\n",
    "                original_img_idx = item_data['image_idx_original']\n",
    "                try:\n",
    "                    # Check if any of the target classes are marked as present for this image.\n",
    "                    if np.any(object_presence_matrix[target_class_original_indices, original_img_idx]):\n",
    "                        filtered_image_data.append(item_data)\n",
    "                except IndexError as e:\n",
    "                    logging.error(f\"IndexError during 'objectPresence' lookup for image index {original_img_idx}. Error: {e}. \"\n",
    "                                  f\"Max original target index: {max(target_class_original_indices) if target_class_original_indices else 'N/A'}. \"\n",
    "                                  \"This image will be excluded by the filter.\")\n",
    "            logging.info(f\"Filtered down to {len(filtered_image_data)} images containing at least one target class.\")\n",
    "    else:\n",
    "        logging.warning(\"'objectPresence' array not found or is invalid in 'index_data'. \"\n",
    "                        \"Efficient filtering for target classes cannot be performed. \"\n",
    "                        \"Consider implementing JSON-based filtering as an alternative if this is problematic. \"\n",
    "                        \"For now, proceeding with data that passed any prior steps (if any).\")\n",
    "        # If filtering is critical and objectPresence is missing, this might mean filtered_image_data remains empty or unchanged.\n",
    "\n",
    "    if not filtered_image_data:\n",
    "        logging.error(\"No images found after filtering step or initial manifest was empty. \"\n",
    "                      \"Subsequent data splitting and processing will be skipped.\")\n",
    "    else:\n",
    "        manifest_df = pd.DataFrame(filtered_image_data)\n",
    "\n",
    "        # --- Split Data into Training, Validation, and Testing Sets ---\n",
    "        logging.info(f\"Attempting to split {len(manifest_df)} filtered images into train/validation/test sets.\")\n",
    "        \n",
    "        # Calculate ratio for the combined validation and test set.\n",
    "        remaining_ratio = VALIDATION_RATIO + TEST_RATIO\n",
    "        if np.isclose(remaining_ratio, 0.0) or remaining_ratio > 1.0: # Basic check\n",
    "             logging.error(f\"Invalid validation/test ratios ({VALIDATION_RATIO}, {TEST_RATIO}). Sum is {remaining_ratio}. Cannot split.\")\n",
    "        else:\n",
    "            stratify_by = None\n",
    "            if 'scene' in manifest_df.columns:\n",
    "                scene_counts = Counter(manifest_df['scene'])\n",
    "                num_unique_scenes = len(scene_counts)\n",
    "                # Heuristic for stratification: requires enough samples in most scene categories.\n",
    "                # sklearn's train_test_split needs at least 2 members for the smallest group in stratification.\n",
    "                scenes_suitable_for_stratify = [s for s, c in scene_counts.items() if c >= max(2, int(1/(VALIDATION_RATIO+TEST_RATIO)))] # Ensure enough for val+test portion\n",
    "                \n",
    "                if len(scenes_suitable_for_stratify) > num_unique_scenes * 0.5 and len(scenes_suitable_for_stratify) > 1:\n",
    "                    stratify_by = manifest_df['scene']\n",
    "                    logging.info(f\"Attempting stratified split by 'scene'. Unique scenes: {num_unique_scenes}. Scenes suitable for stratification: {len(scenes_suitable_for_stratify)}.\")\n",
    "                else:\n",
    "                    logging.warning(f\"Stratification by 'scene' may not be effective (Unique: {num_unique_scenes}, Suitable: {len(scenes_suitable_for_stratify)}). Using random split for train/temp_val_test.\")\n",
    "\n",
    "            try:\n",
    "                # First split: separate training set from a temporary set (validation + test).\n",
    "                train_df, temp_val_test_df = train_test_split(\n",
    "                    manifest_df,\n",
    "                    test_size=remaining_ratio,\n",
    "                    random_state=RANDOM_STATE_SEED,\n",
    "                    stratify=stratify_by\n",
    "                )\n",
    "            except ValueError as e: # Handles cases where stratification fails (e.g., too few samples in a class).\n",
    "                logging.warning(f\"Stratification for train/temp_val_test split failed: {e}. Retrying with random split.\")\n",
    "                train_df, temp_val_test_df = train_test_split(\n",
    "                    manifest_df,\n",
    "                    test_size=remaining_ratio,\n",
    "                    random_state=RANDOM_STATE_SEED,\n",
    "                    stratify=None # Fallback to random split.\n",
    "                )\n",
    "            \n",
    "            # Second split: separate validation and test sets from the temporary set.\n",
    "            if not temp_val_test_df.empty and not np.isclose(remaining_ratio, 0.0):\n",
    "                # Adjust test_size for the second split to be relative to temp_val_test_df.\n",
    "                relative_test_size = TEST_RATIO / remaining_ratio\n",
    "                \n",
    "                val_test_stratify_by = None\n",
    "                if stratify_by is not None : # Attempt to stratify second split if first was stratified\n",
    "                    temp_scene_counts = Counter(temp_val_test_df['scene'])\n",
    "                    temp_scenes_suitable = [s for s,c in temp_scene_counts.items() if c >= 2] # Min 2 for a binary split\n",
    "                    if len(temp_scenes_suitable) > len(temp_scene_counts) * 0.5 and len(temp_scenes_suitable) > 1:\n",
    "                        val_test_stratify_by = temp_val_test_df['scene']\n",
    "                    else:\n",
    "                        logging.warning(\"Stratification for val/test split may not be effective. Using random split for val/test.\")\n",
    "\n",
    "\n",
    "                try:\n",
    "                    val_df, test_df = train_test_split(\n",
    "                        temp_val_test_df,\n",
    "                        test_size=relative_test_size,\n",
    "                        random_state=RANDOM_STATE_SEED,\n",
    "                        stratify=val_test_stratify_by\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    logging.warning(f\"Stratification for val/test split failed: {e}. Retrying with random split.\")\n",
    "                    val_df, test_df = train_test_split(\n",
    "                        temp_val_test_df,\n",
    "                        test_size=relative_test_size,\n",
    "                        random_state=RANDOM_STATE_SEED,\n",
    "                        stratify=None # Fallback to random split.\n",
    "                    )\n",
    "            elif temp_val_test_df.empty:\n",
    "                 logging.warning(\"Temporary validation/test dataframe (temp_val_test_df) is empty. Validation and Test sets will also be empty.\")\n",
    "                 val_df = pd.DataFrame(columns=manifest_df.columns if not manifest_df.empty else None)\n",
    "                 test_df = pd.DataFrame(columns=manifest_df.columns if not manifest_df.empty else None)\n",
    "\n",
    "\n",
    "            logging.info(\"Data splitting process completed.\")\n",
    "            logging.info(f\"  Total images after filtering: {len(manifest_df)}\")\n",
    "            logging.info(f\"  Training set size: {len(train_df)}\")\n",
    "            logging.info(f\"  Validation set size: {len(val_df)}\")\n",
    "            logging.info(f\"  Test set size: {len(test_df)}\")\n",
    "\n",
    "            if train_df.empty or val_df.empty or test_df.empty:\n",
    "                logging.warning(\"One or more dataset splits (train, validation, test) are empty. \"\n",
    "                                \"This may indicate an issue with the filtering, splitting ratios, or very small initial dataset.\")\n",
    "\n",
    "logging.info(\"Cell 3 execution completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be099802-234f-4ac2-bfa9-fe08b8068cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 11:03:15,434 - INFO - Starting Cell 4: Defining YOLOv8 Dataset Conversion Functions.\n",
      "2025-05-12 11:03:15,436 - INFO - Cell 4 execution completed: YOLOv8 conversion functions are now defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: YOLOv8 Dataset Conversion Functions\n",
    "\n",
    "import json\n",
    "from PIL import Image\n",
    "import logging\n",
    "import os # For os.path.exists\n",
    "\n",
    "# Ensure CLASS_NAME_TO_YOLO_ID and TARGET_OBJECT_CLASSES are available from Cell 2.\n",
    "# TARGET_OBJECT_CLASSES should be converted to a set for efficient lookup.\n",
    "TARGET_OBJECT_CLASSES_SET = set(TARGET_OBJECT_CLASSES) if 'TARGET_OBJECT_CLASSES' in globals() and TARGET_OBJECT_CLASSES else set()\n",
    "\n",
    "logging.info(\"Starting Cell 4: Defining YOLOv8 Dataset Conversion Functions.\")\n",
    "\n",
    "def read_json_annotation(json_path):\n",
    "    \"\"\"\n",
    "    Reads and parses a JSON annotation file.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): The full path to the JSON annotation file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed JSON data (specifically the list of object annotations),\n",
    "              or None if the file does not exist or an error occurs during parsing.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(json_path):\n",
    "        logging.warning(f\"Annotation JSON file not found: {json_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(json_path, 'r') as f:\n",
    "            annotation_data = json.load(f)\n",
    "        # The actual object annotations are usually nested, e.g., under 'annotation' -> 'object'\n",
    "        if 'annotation' in annotation_data and 'object' in annotation_data['annotation']:\n",
    "            return annotation_data['annotation']['object']\n",
    "        else:\n",
    "            logging.warning(f\"JSON file {json_path} does not have the expected structure ('annotation' -> 'object').\")\n",
    "            return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        logging.error(f\"Error decoding JSON from file {json_path}: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while reading {json_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def convert_to_yolo_segmentation_format(json_path, image_path, class_name_to_yolo_id_map, target_classes_set):\n",
    "    \"\"\"\n",
    "    Converts ADE20K object annotations from a JSON file to YOLOv8 segmentation format.\n",
    "\n",
    "    Args:\n",
    "        json_path (str): Path to the JSON annotation file.\n",
    "        image_path (str): Path to the corresponding image file (for dimensions).\n",
    "        class_name_to_yolo_id_map (dict): Mapping from class names to YOLO integer IDs.\n",
    "        target_classes_set (set): A set of target class names to include.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of strings, where each string is a YOLO formatted annotation\n",
    "              (class_id norm_x1 norm_y1 norm_x2 norm_y2 ...).\n",
    "              Returns an empty list if no valid objects are found or an error occurs.\n",
    "    \"\"\"\n",
    "    yolo_annotations = []\n",
    "\n",
    "    # 1. Load image to get dimensions for normalization\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Image file not found: {image_path} while processing {json_path}.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error opening image {image_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    if img_width == 0 or img_height == 0:\n",
    "        logging.error(f\"Image {image_path} has zero width or height.\")\n",
    "        return []\n",
    "\n",
    "    # 2. Read object annotations from JSON\n",
    "    object_annotations = read_json_annotation(json_path)\n",
    "    if not object_annotations:\n",
    "        # read_json_annotation would have logged the specific error\n",
    "        return []\n",
    "\n",
    "    # 3. Process each annotated object\n",
    "    for ann_object in object_annotations:\n",
    "        try:\n",
    "            # Use 'name' for the corrected object name, as identified from utils_ade20k.py\n",
    "            object_name = ann_object.get('name', '').strip()\n",
    "            \n",
    "            # Check if it's a primary object (not a part)\n",
    "            # In ADE20K JSON, 'parts' is a dict, and 'part_level' indicates if it's a part.\n",
    "            # Level '0' means it is a main object.\n",
    "            part_level_str = ann_object.get('parts', {}).get('part_level', '-1')\n",
    "            is_primary_object = (int(part_level_str) == 0)\n",
    "\n",
    "            if is_primary_object and object_name in target_classes_set:\n",
    "                yolo_class_id = class_name_to_yolo_id_map.get(object_name)\n",
    "                if yolo_class_id is None:\n",
    "                    # This should not happen if target_classes_set is derived from class_name_to_yolo_id_map keys\n",
    "                    logging.warning(f\"Object name '{object_name}' in target set but not in ID map. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                polygon_data = ann_object.get('polygon')\n",
    "                if not polygon_data:\n",
    "                    logging.debug(f\"Object '{object_name}' in {json_path} has no polygon data. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                polygon_x_coords = polygon_data.get('x', [])\n",
    "                polygon_y_coords = polygon_data.get('y', [])\n",
    "\n",
    "                if not polygon_x_coords or not polygon_y_coords or len(polygon_x_coords) != len(polygon_y_coords):\n",
    "                    logging.debug(f\"Invalid or empty polygon coordinates for '{object_name}' in {json_path}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                if len(polygon_x_coords) < 3: # A polygon must have at least 3 points\n",
    "                    logging.debug(f\"Polygon for '{object_name}' in {json_path} has fewer than 3 points. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # Normalize and format polygon coordinates\n",
    "                normalized_points = []\n",
    "                for px, py in zip(polygon_x_coords, polygon_y_coords):\n",
    "                    norm_x = round(float(px) / img_width, 6)  # Keep reasonable precision\n",
    "                    norm_y = round(float(py) / img_height, 6)\n",
    "                    # Ensure coordinates are within [0, 1] bounds\n",
    "                    norm_x = max(0.0, min(1.0, norm_x))\n",
    "                    norm_y = max(0.0, min(1.0, norm_y))\n",
    "                    normalized_points.extend([norm_x, norm_y])\n",
    "                \n",
    "                if normalized_points:\n",
    "                    yolo_annotations.append(f\"{yolo_class_id} \" + \" \".join(map(str, normalized_points)))\n",
    "            \n",
    "        except (TypeError, ValueError) as e:\n",
    "            logging.warning(f\"Skipping object in {json_path} due to data error: {e}. Object data: {ann_object}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error processing an object in {json_path}: {e}. Object data: {ann_object}\")\n",
    "            \n",
    "    return yolo_annotations\n",
    "\n",
    "# Example usage (for testing purposes, actual use will be in Cell 5)\n",
    "# if 'train_df' in globals() and not train_df.empty:\n",
    "#     sample_item = train_df.iloc[0]\n",
    "#     logging.info(f\"\\n--- Example Conversion Test ---\")\n",
    "#     logging.info(f\"Testing with image: {sample_item['image_path']}\")\n",
    "#     logging.info(f\"Annotation JSON: {sample_item['json_path']}\")\n",
    "#     if TARGET_OBJECT_CLASSES_SET and CLASS_NAME_TO_YOLO_ID:\n",
    "#         example_yolo_data = convert_to_yolo_segmentation_format(\n",
    "#             sample_item['json_path'],\n",
    "#             sample_item['image_path'],\n",
    "#             CLASS_NAME_TO_YOLO_ID,\n",
    "#             TARGET_OBJECT_CLASSES_SET\n",
    "#         )\n",
    "#         if example_yolo_data:\n",
    "#             logging.info(f\"Example YOLO formatted annotations ({len(example_yolo_data)} objects found):\")\n",
    "#             for line in example_yolo_data[:3]: # Print first 3 annotations\n",
    "#                 logging.info(line)\n",
    "#         else:\n",
    "#             logging.info(\"No target objects found or error in example conversion.\")\n",
    "#     else:\n",
    "#         logging.warning(\"TARGET_OBJECT_CLASSES_SET or CLASS_NAME_TO_YOLO_ID not defined for example.\")\n",
    "#     logging.info(f\"--- End Example Conversion Test ---\\n\")\n",
    "\n",
    "\n",
    "logging.info(\"Cell 4 execution completed: YOLOv8 conversion functions are now defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e11d315-8337-451a-a4fd-939eb7582e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 11:03:15,451 - INFO - Starting Cell 5: Generating YOLOv8 Labels and Copying Images.\n",
      "2025-05-12 11:03:15,453 - INFO - Processing 'train' set: 21353 images.\n",
      "2025-05-12 11:03:26,010 - INFO -   Processed 500/21353 images for 'train' set...\n",
      "2025-05-12 11:03:36,109 - INFO -   Processed 1000/21353 images for 'train' set...\n",
      "2025-05-12 11:03:46,299 - INFO -   Processed 1500/21353 images for 'train' set...\n",
      "2025-05-12 11:03:55,759 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/urban/apartment_building__outdoor/ADE_train_00024795.json: 'utf-8' codec can't decode byte 0xf1 in position 83348: invalid continuation byte\n",
      "2025-05-12 11:03:56,716 - INFO -   Processed 2000/21353 images for 'train' set...\n",
      "2025-05-12 11:04:06,830 - INFO -   Processed 2500/21353 images for 'train' set...\n",
      "2025-05-12 11:04:17,038 - INFO -   Processed 3000/21353 images for 'train' set...\n",
      "2025-05-12 11:04:27,400 - INFO -   Processed 3500/21353 images for 'train' set...\n",
      "2025-05-12 11:04:37,611 - INFO -   Processed 4000/21353 images for 'train' set...\n",
      "2025-05-12 11:04:47,672 - INFO -   Processed 4500/21353 images for 'train' set...\n",
      "2025-05-12 11:04:57,738 - INFO -   Processed 5000/21353 images for 'train' set...\n",
      "2025-05-12 11:05:08,134 - INFO -   Processed 5500/21353 images for 'train' set...\n",
      "2025-05-12 11:05:18,319 - INFO -   Processed 6000/21353 images for 'train' set...\n",
      "2025-05-12 11:05:28,528 - INFO -   Processed 6500/21353 images for 'train' set...\n",
      "2025-05-12 11:05:38,610 - INFO -   Processed 7000/21353 images for 'train' set...\n",
      "2025-05-12 11:05:48,764 - INFO -   Processed 7500/21353 images for 'train' set...\n",
      "2025-05-12 11:05:52,631 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/sports_and_leisure/fairway/ADE_train_00007690.json: 'utf-8' codec can't decode byte 0xba in position 11143: invalid start byte\n",
      "2025-05-12 11:05:58,928 - INFO -   Processed 8000/21353 images for 'train' set...\n",
      "2025-05-12 11:06:09,300 - INFO -   Processed 8500/21353 images for 'train' set...\n",
      "2025-05-12 11:06:19,439 - INFO -   Processed 9000/21353 images for 'train' set...\n",
      "2025-05-12 11:06:28,515 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/shopping_and_dining/hardware_store/ADE_train_00008906.json: 'utf-8' codec can't decode byte 0xba in position 3325: invalid start byte\n",
      "2025-05-12 11:06:30,026 - INFO -   Processed 9500/21353 images for 'train' set...\n",
      "2025-05-12 11:06:36,036 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/work_place/conference_room/ADE_train_00020544.json: 'utf-8' codec can't decode byte 0xba in position 19894: invalid start byte\n",
      "2025-05-12 11:06:40,464 - INFO -   Processed 10000/21353 images for 'train' set...\n",
      "2025-05-12 11:06:41,317 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/shopping_and_dining/bookstore/ADE_train_00020434.json: 'utf-8' codec can't decode byte 0xe0 in position 52388: invalid continuation byte\n",
      "2025-05-12 11:06:50,360 - INFO -   Processed 10500/21353 images for 'train' set...\n",
      "2025-05-12 11:07:00,306 - INFO -   Processed 11000/21353 images for 'train' set...\n",
      "2025-05-12 11:07:10,359 - INFO -   Processed 11500/21353 images for 'train' set...\n",
      "2025-05-12 11:07:20,271 - INFO -   Processed 12000/21353 images for 'train' set...\n",
      "2025-05-12 11:07:27,613 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/nature_landscape/picnic_area/ADE_train_00024104.json: 'utf-8' codec can't decode byte 0xf1 in position 28107: invalid continuation byte\n",
      "2025-05-12 11:07:30,215 - INFO -   Processed 12500/21353 images for 'train' set...\n",
      "2025-05-12 11:07:40,100 - INFO -   Processed 13000/21353 images for 'train' set...\n",
      "2025-05-12 11:07:50,335 - INFO -   Processed 13500/21353 images for 'train' set...\n",
      "2025-05-12 11:08:00,318 - INFO -   Processed 14000/21353 images for 'train' set...\n",
      "2025-05-12 11:08:10,291 - INFO -   Processed 14500/21353 images for 'train' set...\n",
      "2025-05-12 11:08:20,202 - INFO -   Processed 15000/21353 images for 'train' set...\n",
      "2025-05-12 11:08:30,301 - INFO -   Processed 15500/21353 images for 'train' set...\n",
      "2025-05-12 11:08:34,192 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/home_or_hotel/kitchen/ADE_train_00021030.json: 'utf-8' codec can't decode byte 0xe7 in position 10910: invalid continuation byte\n",
      "2025-05-12 11:08:40,332 - INFO -   Processed 16000/21353 images for 'train' set...\n",
      "2025-05-12 11:08:50,363 - INFO -   Processed 16500/21353 images for 'train' set...\n",
      "2025-05-12 11:08:57,879 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/urban/castle/ADE_train_00023049.json: 'utf-8' codec can't decode byte 0xf1 in position 16579: invalid continuation byte\n",
      "2025-05-12 11:09:00,274 - INFO -   Processed 17000/21353 images for 'train' set...\n",
      "2025-05-12 11:09:10,428 - INFO -   Processed 17500/21353 images for 'train' set...\n",
      "2025-05-12 11:09:20,487 - INFO -   Processed 18000/21353 images for 'train' set...\n",
      "2025-05-12 11:09:30,446 - INFO -   Processed 18500/21353 images for 'train' set...\n",
      "2025-05-12 11:09:40,489 - INFO -   Processed 19000/21353 images for 'train' set...\n",
      "2025-05-12 11:09:51,763 - INFO -   Processed 19500/21353 images for 'train' set...\n",
      "2025-05-12 11:10:02,999 - INFO -   Processed 20000/21353 images for 'train' set...\n",
      "2025-05-12 11:10:13,638 - INFO -   Processed 20500/21353 images for 'train' set...\n",
      "2025-05-12 11:10:23,777 - INFO -   Processed 21000/21353 images for 'train' set...\n",
      "2025-05-12 11:10:30,763 - INFO - Finished processing 'train' set. Successfully processed: 21353. Errors: 0.\n",
      "2025-05-12 11:10:30,765 - INFO - Processing 'val' set: 2669 images.\n",
      "2025-05-12 11:10:40,803 - INFO -   Processed 500/2669 images for 'val' set...\n",
      "2025-05-12 11:10:50,868 - INFO -   Processed 1000/2669 images for 'val' set...\n",
      "2025-05-12 11:11:00,879 - INFO -   Processed 1500/2669 images for 'val' set...\n",
      "2025-05-12 11:11:10,951 - INFO -   Processed 2000/2669 images for 'val' set...\n",
      "2025-05-12 11:11:21,028 - INFO -   Processed 2500/2669 images for 'val' set...\n",
      "2025-05-12 11:11:24,440 - INFO - Finished processing 'val' set. Successfully processed: 2669. Errors: 0.\n",
      "2025-05-12 11:11:24,440 - INFO - Processing 'test' set: 2670 images.\n",
      "2025-05-12 11:11:34,508 - INFO -   Processed 500/2670 images for 'test' set...\n",
      "2025-05-12 11:11:44,768 - INFO -   Processed 1000/2670 images for 'test' set...\n",
      "2025-05-12 11:11:55,041 - INFO -   Processed 1500/2670 images for 'test' set...\n",
      "2025-05-12 11:12:01,494 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/urban/building_facade/ADE_train_00022980.json: 'utf-8' codec can't decode byte 0xf1 in position 15310: invalid continuation byte\n",
      "2025-05-12 11:12:03,131 - ERROR - An unexpected error occurred while reading ./user-default-efs/ADE20K_2021_17_01/images/ADE/training/urban/campus/ADE_train_00023012.json: 'utf-8' codec can't decode byte 0xf1 in position 23446: invalid continuation byte\n",
      "2025-05-12 11:12:05,303 - INFO -   Processed 2000/2670 images for 'test' set...\n",
      "2025-05-12 11:12:15,361 - INFO -   Processed 2500/2670 images for 'test' set...\n",
      "2025-05-12 11:12:18,953 - INFO - Finished processing 'test' set. Successfully processed: 2670. Errors: 0.\n",
      "2025-05-12 11:12:18,954 - INFO - Cell 5 execution completed: YOLOv8 dataset generation attempted for all splits.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Generate YOLOv8 Labels and Dataset Structure\n",
    "\n",
    "import os\n",
    "import shutil # For copying files\n",
    "import logging\n",
    "import pandas as pd # Expected if train_df, val_df, test_df are pandas DataFrames\n",
    "\n",
    "# Ensure variables from previous cells are available:\n",
    "# - train_df, val_df, test_df (DataFrames from Cell 3)\n",
    "# - CLASS_NAME_TO_YOLO_ID, TARGET_OBJECT_CLASSES_SET (from Cell 2)\n",
    "# - YOLO_IMAGES_DIR, YOLO_LABELS_DIR (from Cell 1)\n",
    "# - convert_to_yolo_segmentation_format (function from Cell 4)\n",
    "\n",
    "logging.info(\"Starting Cell 5: Generating YOLOv8 Labels and Copying Images.\")\n",
    "\n",
    "def process_dataset_split(dataframe, subset_name, class_to_id_map, target_classes_set,\n",
    "                          yolo_images_root, yolo_labels_root):\n",
    "    \"\"\"\n",
    "    Processes a specific dataset split (train, val, or test).\n",
    "    Generates YOLO label files and copies images to the target YOLO directory structure.\n",
    "\n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): DataFrame containing 'image_path' and 'json_path' for the split.\n",
    "        subset_name (str): The name of the subset (e.g., 'train', 'val', 'test').\n",
    "        class_to_id_map (dict): Mapping from class names to YOLO integer IDs.\n",
    "        target_classes_set (set): A set of target class names.\n",
    "        yolo_images_root (str): Root directory for YOLO images.\n",
    "        yolo_labels_root (str): Root directory for YOLO labels.\n",
    "    \"\"\"\n",
    "    if not isinstance(dataframe, pd.DataFrame) or dataframe.empty:\n",
    "        logging.warning(f\"DataFrame for '{subset_name}' is empty or not a DataFrame. Skipping processing for this subset.\")\n",
    "        return\n",
    "\n",
    "    target_img_dir = os.path.join(yolo_images_root, subset_name)\n",
    "    target_lbl_dir = os.path.join(yolo_labels_root, subset_name)\n",
    "\n",
    "    # Ensure target directories exist (should have been created in Cell 1)\n",
    "    os.makedirs(target_img_dir, exist_ok=True)\n",
    "    os.makedirs(target_lbl_dir, exist_ok=True)\n",
    "\n",
    "    logging.info(f\"Processing '{subset_name}' set: {len(dataframe)} images.\")\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for index, row in dataframe.iterrows():\n",
    "        try:\n",
    "            image_path_original = row['image_path']\n",
    "            json_path_original = row['json_path']\n",
    "\n",
    "            if not os.path.exists(image_path_original):\n",
    "                logging.warning(f\"Original image file not found: {image_path_original}. Skipping.\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "            # json_path existence is checked within convert_to_yolo_segmentation_format\n",
    "\n",
    "            # 1. Convert annotations to YOLO format\n",
    "            yolo_annotation_strings = convert_to_yolo_segmentation_format(\n",
    "                json_path_original,\n",
    "                image_path_original,\n",
    "                class_to_id_map,\n",
    "                target_classes_set\n",
    "            )\n",
    "\n",
    "            # 2. Determine output paths\n",
    "            base_image_filename = os.path.basename(image_path_original)\n",
    "            label_filename = os.path.splitext(base_image_filename)[0] + \".txt\"\n",
    "            \n",
    "            output_label_path = os.path.join(target_lbl_dir, label_filename)\n",
    "            output_image_path = os.path.join(target_img_dir, base_image_filename)\n",
    "\n",
    "            # 3. Write label file (even if empty, as YOLO expects it)\n",
    "            with open(output_label_path, 'w') as f:\n",
    "                for line in yolo_annotation_strings:\n",
    "                    f.write(line + \"\\n\")\n",
    "\n",
    "            # 4. Copy image file\n",
    "            shutil.copy(image_path_original, output_image_path)\n",
    "            \n",
    "            processed_count += 1\n",
    "            if processed_count % 500 == 0: # Log progress every 500 images\n",
    "                logging.info(f\"  Processed {processed_count}/{len(dataframe)} images for '{subset_name}' set...\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to process item {index} (image: {row.get('image_path', 'N/A')}) in '{subset_name}' set: {e}\")\n",
    "            error_count += 1\n",
    "            # Continue with the next file\n",
    "\n",
    "    logging.info(f\"Finished processing '{subset_name}' set. Successfully processed: {processed_count}. Errors: {error_count}.\")\n",
    "\n",
    "\n",
    "# Check if necessary DataFrames are defined and not empty\n",
    "data_splits_to_process = {}\n",
    "if 'train_df' in globals() and isinstance(train_df, pd.DataFrame) and not train_df.empty:\n",
    "    data_splits_to_process['train'] = train_df\n",
    "else:\n",
    "    logging.warning(\"train_df is not defined or is empty. Training set will not be processed.\")\n",
    "\n",
    "if 'val_df' in globals() and isinstance(val_df, pd.DataFrame) and not val_df.empty:\n",
    "    data_splits_to_process['val'] = val_df\n",
    "else:\n",
    "    logging.warning(\"val_df is not defined or is empty. Validation set will not be processed.\")\n",
    "\n",
    "if 'test_df' in globals() and isinstance(test_df, pd.DataFrame) and not test_df.empty:\n",
    "    data_splits_to_process['test'] = test_df\n",
    "else:\n",
    "    logging.warning(\"test_df is not defined or is empty. Test set will not be processed.\")\n",
    "\n",
    "# Check for critical supporting variables from Cell 2\n",
    "if 'CLASS_NAME_TO_YOLO_ID' not in globals() or not CLASS_NAME_TO_YOLO_ID:\n",
    "    logging.error(\"CLASS_NAME_TO_YOLO_ID is not defined or empty. Cannot proceed with label generation.\")\n",
    "elif 'TARGET_OBJECT_CLASSES_SET' not in globals() or not TARGET_OBJECT_CLASSES_SET:\n",
    "    logging.error(\"TARGET_OBJECT_CLASSES_SET is not defined or empty. Cannot proceed with label generation.\")\n",
    "else:\n",
    "    # Process each dataset split\n",
    "    for subset_name, dataframe_to_process in data_splits_to_process.items():\n",
    "        process_dataset_split(\n",
    "            dataframe_to_process,\n",
    "            subset_name,\n",
    "            CLASS_NAME_TO_YOLO_ID,\n",
    "            TARGET_OBJECT_CLASSES_SET,\n",
    "            YOLO_IMAGES_DIR, # Defined in Cell 1\n",
    "            YOLO_LABELS_DIR  # Defined in Cell 1\n",
    "        )\n",
    "\n",
    "logging.info(\"Cell 5 execution completed: YOLOv8 dataset generation attempted for all splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e00617-8ebe-49f9-a347-0cb7248188cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:29:17,780 - INFO - Starting Cell 6: Creating YOLOv8 Dataset YAML File.\n",
      "2025-05-13 01:29:17,783 - INFO - Successfully created YOLOv8 dataset YAML file at: ./ADE20K_YOLOv8_Dataset/ade20k_yolo.yaml\n",
      "2025-05-13 01:29:17,784 - INFO - YAML file content:\n",
      "2025-05-13 01:29:17,785 - INFO - \n",
      "path: /home/sagemaker-user/ADE20K_YOLOv8_Dataset\n",
      "train: images/train\n",
      "val: images/val\n",
      "test: images/test\n",
      "nc: 72\n",
      "names: ['airplane, aeroplane, plane', 'backpack, back pack, knapsack, packsack, rucksack,\n",
      "    haversack', bed, bench, 'bicycle, bike, wheel, cycle', bird, blender, boat, book,\n",
      "  bottle, bowl, building, 'bus, autobus, coach, charabanc, double-decker, jitney,\n",
      "    motorbus, motorcoach, omnibus, passenger vehicle', cabinet, car, cat, ceiling,\n",
      "  chair, clock, 'cow, moo-cow', cup, cushion, desk, 'dog, domestic dog, Canis familiaris',\n",
      "  door, fence, floor, fork, handbag, 'horse, Equus caballus', house, keyboard, knife,\n",
      "  'laptop, laptop computer', light, light source, microwave, monitor, mouse, oven,\n",
      "  parking meter, person, pillow, plate, pole, 'refrigerator, icebox', 'remote control,\n",
      "    remote', road, scissors, sheep, sidewalk, 'signboard, sign', sink, skateboard,\n",
      "  sky, 'sofa, couch, lounge', spoon, 'stairs, steps', street sign, table, teddy bear,\n",
      "  tennis racket, toaster, 'toilet, can, commode, crapper, pot, potty, stool, throne',\n",
      "  toothbrush, 'traffic light, traffic signal, stoplight', tree, 'truck, motortruck',\n",
      "  umbrella, vase, wall, window]\n",
      "\n",
      "2025-05-13 01:29:17,785 - INFO - Cell 6 execution completed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Create YOLOv8 Dataset YAML File\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import yaml # PyYAML library for YAML manipulation\n",
    "\n",
    "# Ensure variables from previous cells are available:\n",
    "# - YOLO_DATASET_ROOT_DIR (from Cell 1)\n",
    "# - YOLO_ID_TO_CLASS_NAME (from Cell 2, mapping integer ID to class name string)\n",
    "# - TARGET_OBJECT_CLASSES (from Cell 2, list of class name strings in order)\n",
    "\n",
    "logging.info(\"Starting Cell 6: Creating YOLOv8 Dataset YAML File.\")\n",
    "\n",
    "# Define the name and path for the YAML file.\n",
    "# It's common to place this in the root of the YOLO dataset directory.\n",
    "yaml_file_name = \"ade20k_yolo.yaml\"\n",
    "yaml_file_path = os.path.join(YOLO_DATASET_ROOT_DIR, yaml_file_name)\n",
    "\n",
    "# Check if critical variables are defined\n",
    "if 'YOLO_DATASET_ROOT_DIR' not in globals() or not YOLO_DATASET_ROOT_DIR:\n",
    "    logging.error(\"YOLO_DATASET_ROOT_DIR is not defined. Cannot create YAML file.\")\n",
    "elif ('YOLO_ID_TO_CLASS_NAME' not in globals() or not YOLO_ID_TO_CLASS_NAME or\n",
    "      'TARGET_OBJECT_CLASSES' not in globals() or not TARGET_OBJECT_CLASSES):\n",
    "    logging.error(\"Class name mappings (YOLO_ID_TO_CLASS_NAME or TARGET_OBJECT_CLASSES) \"\n",
    "                  \"are not defined or empty. Cannot create YAML file.\")\n",
    "else:\n",
    "    try:\n",
    "        # Ensure YOLO_DATASET_ROOT_DIR is an absolute path for robustness in the YAML file.\n",
    "        abs_yolo_dataset_root_dir = os.path.abspath(YOLO_DATASET_ROOT_DIR)\n",
    "\n",
    "        # Define paths relative to the YOLO_DATASET_ROOT_DIR for train/val/test sets.\n",
    "        # These are standard subdirectories created in Cell 1 and populated in Cell 5.\n",
    "        train_images_rel_path = os.path.join('images', 'train')\n",
    "        val_images_rel_path = os.path.join('images', 'val')\n",
    "        test_images_rel_path = os.path.join('images', 'test')\n",
    "\n",
    "        # Prepare the list of class names in the correct order (index = class_id).\n",
    "        # TARGET_OBJECT_CLASSES should already be sorted if it was derived from a sorted list\n",
    "        # or if CLASS_NAME_TO_YOLO_ID was built from it sequentially.\n",
    "        # YOLO_ID_TO_CLASS_NAME provides the mapping {0: name0, 1: name1, ...}\n",
    "        # The number of classes is the length of TARGET_OBJECT_CLASSES.\n",
    "        num_classes = len(TARGET_OBJECT_CLASSES)\n",
    "        class_names_list = [None] * num_classes\n",
    "        for i in range(num_classes):\n",
    "            class_name = YOLO_ID_TO_CLASS_NAME.get(i)\n",
    "            if class_name is None:\n",
    "                logging.error(f\"Error: Class ID {i} not found in YOLO_ID_TO_CLASS_NAME map. Inconsistent state.\")\n",
    "                raise ValueError(f\"Missing class name for ID {i} in YOLO_ID_TO_CLASS_NAME.\")\n",
    "            class_names_list[i] = class_name\n",
    "        \n",
    "        # Construct the data dictionary for the YAML file.\n",
    "        yaml_data = {\n",
    "            'path': abs_yolo_dataset_root_dir,  # Absolute path to the dataset root\n",
    "            'train': train_images_rel_path,     # Path to train images relative to 'path'\n",
    "            'val': val_images_rel_path,         # Path to val images relative to 'path'\n",
    "            'test': test_images_rel_path,       # Path to test images relative to 'path'\n",
    "            'nc': num_classes,                  # Number of classes\n",
    "            'names': class_names_list           # List of class names\n",
    "        }\n",
    "\n",
    "        # Write the dictionary to a YAML file.\n",
    "        # Using 'sort_keys=False' to maintain the order as defined in yaml_data.\n",
    "        # 'default_flow_style=None' makes lists appear on new lines typically.\n",
    "        with open(yaml_file_path, 'w') as f:\n",
    "            yaml.dump(yaml_data, f, sort_keys=False, default_flow_style=None)\n",
    "\n",
    "        logging.info(f\"Successfully created YOLOv8 dataset YAML file at: {yaml_file_path}\")\n",
    "        logging.info(\"YAML file content:\")\n",
    "        # Reading and printing the file content for verification.\n",
    "        with open(yaml_file_path, 'r') as f_read:\n",
    "            logging.info(f\"\\n{f_read.read()}\")\n",
    "\n",
    "    except ImportError:\n",
    "        logging.error(\"The 'PyYAML' library is not installed. Cannot write YAML file using yaml.dump().\")\n",
    "        logging.info(\"Attempting to write YAML content as a formatted string as a fallback.\")\n",
    "        # Fallback: Manually create YAML string content if PyYAML is not available.\n",
    "        # This is less robust than using the PyYAML library but avoids an external dependency if not present.\n",
    "        yaml_string_content = f\"path: {abs_yolo_dataset_root_dir}\\n\"\n",
    "        yaml_string_content += f\"train: {train_images_rel_path}\\n\"\n",
    "        yaml_string_content += f\"val: {val_images_rel_path}\\n\"\n",
    "        yaml_string_content += f\"test: {test_images_rel_path}\\n\\n\"\n",
    "        yaml_string_content += f\"nc: {num_classes}\\n\"\n",
    "        yaml_string_content += \"names:\\n\"\n",
    "        for i, name in enumerate(class_names_list):\n",
    "            yaml_string_content += f\"  - \\\"{name}\\\" # ID {i}\\n\" # Enclosing names in quotes if they contain special chars.\n",
    "        \n",
    "        try:\n",
    "            with open(yaml_file_path, 'w') as f_str:\n",
    "                f_str.write(yaml_string_content)\n",
    "            logging.info(f\"Successfully wrote YAML content as string to: {yaml_file_path}\")\n",
    "            logging.info(\"Fallback YAML file content:\")\n",
    "            logging.info(f\"\\n{yaml_string_content}\")\n",
    "        except Exception as e_str:\n",
    "            logging.error(f\"Failed to write YAML content as string: {e_str}\")\n",
    "\n",
    "    except ValueError as ve: # Catch potential ValueError from class name list construction\n",
    "        logging.error(f\"A ValueError occurred during YAML data preparation: {ve}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while creating the YAML file: {e}\")\n",
    "\n",
    "logging.info(\"Cell 6 execution completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f1c16d-d10b-4cb1-a3fe-ad773fe8fd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:29:21,494 - INFO - Starting Cell 7: Setting up Ultralytics YOLOv8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /opt/conda/lib/python3.12/site-packages (8.3.133)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (11.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.12/site-packages (from ultralytics) (5.9.8)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/conda/lib/python3.12/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (75.8.2)\n",
      "Requirement already satisfied: sympy!=1.13.2,>=1.13.1 in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from sympy!=1.13.2,>=1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease    \n",
      "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease              \n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "libgl1-mesa-glx is already the newest version (23.0.4-0ubuntu1~22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:29:29,039 - INFO - Successfully imported YOLO from ultralytics.\n",
      "2025-05-13 01:29:29,040 - INFO - Cell 7 execution completed: Ultralytics YOLOv8 setup attempted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Setup Ultralytics YOLOv8\n",
    "\n",
    "import logging\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "logging.info(\"Starting Cell 7: Setting up Ultralytics YOLOv8.\")\n",
    "\n",
    "!pip install ultralytics\n",
    "!sudo apt-get update && sudo apt-get install -y libgl1-mesa-glx\n",
    "\n",
    "# --- Installation Check and Attempt (Optional) ---\n",
    "# The Ultralytics package is required. If it's not already installed in the\n",
    "# environment, the following line can be uncommented and run.\n",
    "# In managed environments like SageMaker, it might be pre-installed or\n",
    "# require a specific installation procedure.\n",
    "\n",
    "# try:\n",
    "#     logging.info(\"Checking if ultralytics is installed...\")\n",
    "#     import ultralytics\n",
    "#     logging.info(f\"Ultralytics YOLOv8 already installed. Version: {ultralytics.__version__}\")\n",
    "# except ImportError:\n",
    "#     logging.warning(\"Ultralytics YOLOv8 not found. Attempting to install...\")\n",
    "#     try:\n",
    "#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"ultralytics\"])\n",
    "#         logging.info(\"Successfully installed ultralytics.\")\n",
    "#         import ultralytics # Verify import after installation\n",
    "#         logging.info(f\"Ultralytics YOLOv8 successfully imported after installation. Version: {ultralytics.__version__}\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Failed to install or import ultralytics: {e}\")\n",
    "#         logging.error(\"Please ensure 'ultralytics' is installed in the environment to proceed with model training.\")\n",
    "#         # Raising an error here might be too disruptive, logging is preferred for now.\n",
    "#         # The import attempt below will ultimately determine if it's usable.\n",
    "\n",
    "# --- Import YOLO Class ---\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    logging.info(\"Successfully imported YOLO from ultralytics.\")\n",
    "    # Displaying YOLOv8 environment details (optional)\n",
    "    # from ultralytics.utils.checks import check_requirements, check_version, check_pytorch # check_yolo\n",
    "    # check_requirements('ultralytics') # This can be verbose\n",
    "    # check_yolo() # This prints a lot of info, might be too much for standard log.\n",
    "except ImportError as e:\n",
    "    logging.error(f\"Failed to import YOLO from ultralytics: {e}\")\n",
    "    logging.error(\"This usually means 'ultralytics' is not installed correctly or there's an environment issue.\")\n",
    "    logging.error(\"Please install it using 'pip install ultralytics' and ensure the Python environment is configured correctly.\")\n",
    "    # It might be prudent to raise an exception here if YOLO cannot be imported,\n",
    "    # as subsequent cells will fail. For now, logging the error.\n",
    "    # raise e # Uncomment to make this cell fail if import fails.\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred during YOLO import: {e}\")\n",
    "    # raise e\n",
    "\n",
    "logging.info(\"Cell 7 execution completed: Ultralytics YOLOv8 setup attempted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ba69a-dab9-4b62-9813-4309fafc6356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-12 11:12:28,935 - INFO - Starting Cell 8: Training YOLOv8s-seg Model.\n",
      "2025-05-12 11:12:29,118 - INFO - CUDA GPU detected. Training will run on device: 0\n",
      "2025-05-12 11:12:29,119 - INFO - Using dataset YAML configuration from: ./ADE20K_YOLOv8_Dataset/ade20k_yolo.yaml\n",
      "2025-05-12 11:12:29,467 - INFO - Initialized YOLOv8s-seg model with pre-trained weights (yolov8s-seg.pt).\n",
      "2025-05-12 11:12:29,468 - INFO - Starting model training with the following parameters:\n",
      "2025-05-12 11:12:29,469 - INFO -   Data YAML: ./ADE20K_YOLOv8_Dataset/ade20k_yolo.yaml\n",
      "2025-05-12 11:12:29,469 - INFO -   Epochs: 150\n",
      "2025-05-12 11:12:29,470 - INFO -   Image Size: 640\n",
      "2025-05-12 11:12:29,470 - INFO -   Batch Size: 16\n",
      "2025-05-12 11:12:29,471 - INFO -   Device: 0\n",
      "2025-05-12 11:12:29,472 - INFO -   Project Directory: ./YOLOv8_Training_Runs_ADE20K\n",
      "2025-05-12 11:12:29,473 - INFO -   Run Name: ade20k_yolov8s_seg_run1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.131 🚀 Python-3.12.9 torch-2.5.1 CUDA:0 (Tesla T4, 14918MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=./ADE20K_YOLOv8_Dataset/ade20k_yolo.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=150, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=ade20k_yolov8s_seg_run12, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=./YOLOv8_Training_Runs_ADE20K, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=72\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2798408  ultralytics.nn.modules.head.Segment          [72, 32, 128, [128, 256, 512]]\n",
      "YOLOv8s-seg summary: 151 layers, 11,817,960 parameters, 11,817,944 gradients, 42.8 GFLOPs\n",
      "\n",
      "Transferred 411/417 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1010.1±2011.1 MB/s, size: 243.9 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/sagemaker-user/ADE20K_YOLOv8_Dataset/labels/train.cache... 21353 images, 142 backgrounds, 0 corrupt: 100%|██████████| 21353/21353 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 750.3±281.3 MB/s, size: 173.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/sagemaker-user/ADE20K_YOLOv8_Dataset/labels/val.cache... 2669 images, 15 backgrounds, 0 corrupt: 100%|██████████| 2669/2669 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/12 11:12:47 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2025/05/12 11:12:47 INFO mlflow.tracking.fluent: Autologging successfully enabled for statsmodels.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(91835236bc18473db8a2679dc15a4819) to runs/mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs/mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mYOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12\u001b[0m\n",
      "Starting training for 150 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/150      5.42G      1.178      2.843      2.266      1.312         92        640: 100%|██████████| 1335/1335 [08:29<00:00,  2.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 84/84 [00:33<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2669      19046      0.612      0.247      0.251      0.175      0.603      0.239      0.239      0.142\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/150      5.68G       1.13      2.653      1.672      1.282         77        640: 100%|██████████| 1335/1335 [08:19<00:00,  2.67it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 84/84 [00:32<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2669      19046      0.436       0.23      0.225      0.148      0.409      0.215      0.207      0.119\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/150       5.9G      1.213      2.797      1.765      1.337        126        640: 100%|██████████| 1335/1335 [08:16<00:00,  2.69it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 84/84 [00:32<00:00,  2.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2669      19046      0.475      0.186      0.184      0.116      0.463      0.173      0.167     0.0886\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/150      7.01G      1.268      2.889      1.874      1.387        209        640:  11%|█         | 147/1335 [00:54<07:17,  2.72it/s]"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train YOLOv8s-seg Model\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import torch # To check CUDA availability\n",
    "\n",
    "# Ensure YOLO class is imported (from Cell 7) and yaml_file_path is defined (from Cell 6)\n",
    "\n",
    "logging.info(\"Starting Cell 8: Training YOLOv8s-seg Model.\")\n",
    "\n",
    "# --- Training Configuration ---\n",
    "# These parameters can be adjusted based on requirements and available resources.\n",
    "TRAINING_EPOCHS = 150  # Number of epochs to train for. Start with a moderate number.\n",
    "IMAGE_SIZE = 640      # Input image size for training.\n",
    "BATCH_SIZE = 16       # Batch size. Adjust based on GPU memory. (e.g., 8, 16, 32)\n",
    "# Check if CUDA is available and set the device accordingly.\n",
    "DEVICE = '0' if torch.cuda.is_available() else 'cpu' # Use GPU '0' if available, else CPU.\n",
    "if DEVICE == 'cpu':\n",
    "    logging.warning(\"CUDA GPU not detected by PyTorch. Training will run on CPU, which will be very slow.\")\n",
    "    logging.warning(\"If a GPU is present, please ensure PyTorch is correctly installed with CUDA support and the GPU drivers are up to date.\")\n",
    "else:\n",
    "    logging.info(f\"CUDA GPU detected. Training will run on device: {DEVICE}\")\n",
    "\n",
    "# Define project and run names for organizing training outputs.\n",
    "# YOLOv8 saves runs under '{PROJECT_DIR}/{RUN_NAME}'\n",
    "PROJECT_DIR = os.path.join('.', 'YOLOv8_Training_Runs_ADE20K') # Root for all training runs for this project\n",
    "RUN_NAME = 'ade20k_yolov8s_seg_run12' # Specific name for this particular training run\n",
    "\n",
    "# Path to the dataset configuration YAML file (created in Cell 6).\n",
    "# Ensure 'yaml_file_path' is correctly carried over from Cell 6.\n",
    "if 'yaml_file_path' not in globals() or not os.path.exists(yaml_file_path):\n",
    "    logging.error(f\"Dataset YAML configuration file not found at expected path: {globals().get('yaml_file_path', 'Path not defined')}.\")\n",
    "    logging.error(\"Please ensure Cell 6 was executed successfully and yaml_file_path is correct.\")\n",
    "    # Halt further execution in this cell if YAML is missing.\n",
    "    # This can be done by raising an error or simply not proceeding.\n",
    "    # For now, we'll log and let it potentially fail at model.train() if path is None.\n",
    "    dataset_yaml_path = None\n",
    "else:\n",
    "    dataset_yaml_path = yaml_file_path\n",
    "    logging.info(f\"Using dataset YAML configuration from: {dataset_yaml_path}\")\n",
    "\n",
    "# --- Initialize and Train the Model ---\n",
    "if dataset_yaml_path:\n",
    "    try:\n",
    "        # Load a pre-trained YOLOv8s segmentation model.\n",
    "        # Using '.pt' loads pre-trained weights, which is recommended for custom datasets.\n",
    "        model = YOLO('yolov8s-seg.pt')\n",
    "        logging.info(\"Initialized YOLOv8s-seg model with pre-trained weights (yolov8s-seg.pt).\")\n",
    "\n",
    "        logging.info(f\"Starting model training with the following parameters:\")\n",
    "        logging.info(f\"  Data YAML: {dataset_yaml_path}\")\n",
    "        logging.info(f\"  Epochs: {TRAINING_EPOCHS}\")\n",
    "        logging.info(f\"  Image Size: {IMAGE_SIZE}\")\n",
    "        logging.info(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "        logging.info(f\"  Device: {DEVICE}\")\n",
    "        logging.info(f\"  Project Directory: {PROJECT_DIR}\")\n",
    "        logging.info(f\"  Run Name: {RUN_NAME}\")\n",
    "        \n",
    "        # Start training.\n",
    "        # The training process will output logs to the console and save results\n",
    "        # (weights, metrics, plots) in 'PROJECT_DIR/RUN_NAME'.\n",
    "        # Other arguments like 'workers', 'patience' (for early stopping) can be added.\n",
    "        model.train(\n",
    "            data=dataset_yaml_path,\n",
    "            epochs=TRAINING_EPOCHS,\n",
    "            imgsz=IMAGE_SIZE,\n",
    "            batch=BATCH_SIZE,\n",
    "            device=DEVICE,\n",
    "            project=PROJECT_DIR,\n",
    "            name=RUN_NAME,\n",
    "            patience=20, # Number of epochs to wait for improvement before early stopping (e.g., 20-30)\n",
    "            exist_ok=False # If True, allows overwriting existing run with same name. Set to False to prevent accidental overwrites.\n",
    "                           # If False and run exists, it will create e.g., run_name2, run_name3 ...\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Model training initiated. Check console output for progress.\")\n",
    "        logging.info(f\"Training results, including weights and metrics, will be saved in: \"\n",
    "                     f\"{os.path.abspath(os.path.join(PROJECT_DIR, RUN_NAME))}\")\n",
    "\n",
    "    except NameError as ne: # Catch if YOLO class was not imported\n",
    "        logging.error(f\"NameError: {ne}. This might indicate that the YOLO class was not successfully imported in Cell 7.\")\n",
    "    except FileNotFoundError as fnfe: # Catch if model weights or data YAML not found\n",
    "        logging.error(f\"FileNotFoundError: {fnfe}. Ensure 'yolov8s-seg.pt' is accessible and data YAML path is correct.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during model initialization or training: {e}\")\n",
    "else:\n",
    "    logging.error(\"Cannot start training as dataset YAML path is not available.\")\n",
    "\n",
    "logging.info(\"Cell 8 execution completed: Model training process has been started (if all configurations were correct).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792dfebf-4da2-48ef-b60d-551c0610ecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # Add this if not already imported in this new session\n",
    "IMAGE_SIZE = 640      # Image size used during the 88-epoch training\n",
    "BATCH_SIZE = 16       # Batch size used (can be different for eval, but consistency is fine)\n",
    "DEVICE = '0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Ensure PROJECT_DIR and RUN_NAME are correctly defined here if not in Cell 1\n",
    "# PROJECT_DIR = './YOLOv8_Training_Runs_ADE20K' # Already defined if you added to Cell 1\n",
    "# RUN_NAME = 'ade20k_yolov8s_seg_run1'      # Already defined if you added to Cell 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6046fb-2243-438e-a350-4ce538a5a551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:29:39,725 - INFO - Starting Cell 9: Loading best trained model and evaluating.\n",
      "2025-05-13 01:29:39,726 - INFO - Path to the best model weights: ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.pt\n",
      "2025-05-13 01:29:40,187 - INFO - Successfully loaded the best trained model from: ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.pt\n",
      "2025-05-13 01:29:40,187 - INFO - Using dataset YAML for evaluation: ./ADE20K_YOLOv8_Dataset/ade20k_yolo.yaml\n",
      "2025-05-13 01:29:40,188 - INFO - Evaluation parameters: Image Size=640, Batch Size=16, Device=0\n",
      "2025-05-13 01:29:40,188 - INFO - Starting evaluation on the validation set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.133 🚀 Python-3.12.9 torch-2.5.1 CUDA:0 (Tesla T4, 14918MiB)\n",
      "YOLOv8s-seg summary (fused): 85 layers, 11,807,464 parameters, 0 gradients, 42.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2236.1±1031.5 MB/s, size: 216.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/sagemaker-user/ADE20K_YOLOv8_Dataset/labels/val.cache... 2669 images, 15 backgrounds, 0 corrupt: 100%|██████████| 2669/2669 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   1%|          | 1/167 [00:00<01:25,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   1%|          | 2/167 [00:01<01:26,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 167/167 [00:41<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2669      19046      0.531      0.367      0.374      0.267      0.518      0.359      0.355       0.22\n",
      "airplane, aeroplane, plane         24         45      0.648      0.556      0.611      0.381      0.672      0.578      0.619      0.313\n",
      "backpack, back pack, knapsack, packsack, rucksack, haversack         25         37     0.0941      0.027     0.0197     0.0106     0.0929      0.027     0.0261    0.00939\n",
      "                   bed        227        262      0.846      0.941      0.956      0.848      0.842      0.939      0.949      0.736\n",
      "                 bench         82        160      0.382      0.212      0.208      0.132      0.325      0.181      0.146     0.0722\n",
      "bicycle, bike, wheel, cycle         34         53      0.483      0.604      0.483      0.305      0.404      0.509      0.379      0.159\n",
      "                  bird         14         34      0.489       0.17      0.217      0.142      0.406      0.141      0.175     0.0901\n",
      "               blender          4          4      0.384       0.25      0.281       0.16      0.377       0.25      0.267      0.135\n",
      "                  boat         54        124      0.426      0.245      0.256      0.144      0.377       0.22      0.241      0.126\n",
      "                  book        139        674      0.494      0.457      0.441      0.264      0.476      0.443      0.421       0.23\n",
      "                bottle        142        618       0.65      0.262      0.306      0.167      0.618      0.251      0.281      0.119\n",
      "                  bowl         80        128       0.39      0.328      0.228       0.12      0.378       0.32      0.215      0.102\n",
      "bus, autobus, coach, charabanc, double-decker, jitney, motorbus, motorcoach, omnibus, passenger vehicle         25         45      0.521        0.4       0.44       0.33      0.491      0.378      0.407      0.283\n",
      "               cabinet        418       1010      0.526      0.611      0.573      0.423       0.54      0.632      0.588      0.395\n",
      "               ceiling        820        994      0.797      0.803      0.863      0.708      0.787      0.794      0.841      0.584\n",
      "                 chair        434       1561      0.593      0.629      0.605      0.439      0.574      0.613      0.572      0.308\n",
      "                 clock        113        122      0.444      0.328      0.353       0.23       0.44      0.336      0.345      0.189\n",
      "          cow, moo-cow          2         13      0.312      0.384      0.237      0.162       0.31      0.385      0.237      0.162\n",
      "                   cup         26         51      0.397     0.0784     0.0918     0.0514       0.39     0.0784        0.1     0.0565\n",
      "               cushion        211        671      0.684      0.723      0.746      0.566      0.683      0.724      0.735      0.513\n",
      "                  desk         72        130      0.416      0.338      0.344      0.235      0.402      0.331      0.316      0.137\n",
      "dog, domestic dog, Canis familiaris          8         14       0.47      0.214      0.252      0.208      0.465      0.214      0.241      0.183\n",
      "                  door        377        509      0.511      0.448      0.446      0.303      0.493       0.44      0.435      0.272\n",
      "                  fork          7         11          1          0     0.0439     0.0189          1          0    0.00253   0.000691\n",
      "               handbag          1          1          0          0          0          0          0          0          0          0\n",
      " horse, Equus caballus          5         16      0.199       0.25       0.15     0.0691      0.149      0.188     0.0815     0.0315\n",
      "                 house         89        170      0.448        0.3      0.261      0.181      0.437      0.297      0.248       0.15\n",
      "              keyboard          6          8          0          0    0.00944     0.0043          0          0    0.00944    0.00343\n",
      "                 knife         18         19      0.875      0.211      0.359      0.235      0.857      0.211      0.366      0.202\n",
      "laptop, laptop computer         23         34      0.767      0.353      0.411       0.31      0.764      0.353       0.41        0.3\n",
      "                  oven         37         48      0.664      0.646       0.62       0.47      0.658      0.646      0.619      0.428\n",
      "         parking meter         12         16      0.612        0.5      0.476      0.223      0.609        0.5      0.477       0.24\n",
      "                pillow        113        253      0.557      0.696      0.676      0.493      0.548      0.688      0.653      0.442\n",
      "                 plate         73        259      0.512      0.397      0.379      0.235      0.469      0.367      0.328      0.176\n",
      "                  pole        107        201      0.373      0.133      0.133      0.072      0.377      0.139      0.135     0.0662\n",
      "  refrigerator, icebox         62         63      0.635      0.778        0.8      0.673       0.62      0.762      0.788      0.633\n",
      "remote control, remote         22         26          0          0     0.0277    0.00849          0          0     0.0166    0.00681\n",
      "              scissors          3          3          1          0          0          0          1          0          0          0\n",
      "                 sheep          2          5          1          0     0.0259     0.0152          1          0     0.0259     0.0138\n",
      "       signboard, sign        326        602      0.396      0.317      0.297      0.179      0.401      0.327      0.287      0.135\n",
      "                  sink        162        180      0.635      0.717      0.722      0.509      0.633      0.717      0.734      0.486\n",
      "                   sky       1193       1247       0.83      0.891      0.916      0.808      0.815      0.877      0.891       0.69\n",
      "   sofa, couch, lounge        160        218      0.677      0.748      0.782      0.675      0.646       0.72      0.744      0.472\n",
      "                 spoon          6         11          1          0      0.107     0.0258          1          0     0.0168    0.00654\n",
      "         stairs, steps        103        120      0.239       0.15     0.0993     0.0493      0.256      0.163     0.0911     0.0424\n",
      "           street sign          4          4          1          0    0.00554    0.00444          1          0    0.00554    0.00554\n",
      "                 table        579       1027      0.693      0.632      0.688       0.51      0.652      0.596      0.617      0.322\n",
      "               toaster         12         12      0.584      0.417      0.466      0.373       0.58      0.417      0.476      0.304\n",
      "toilet, can, commode, crapper, pot, potty, stool, throne         45         46      0.609      0.891      0.818      0.731      0.605      0.891      0.847      0.637\n",
      "            toothbrush          3          4          0          0          0          0          0          0          0          0\n",
      "traffic light, traffic signal, stoplight         67        139      0.324      0.187      0.181     0.0895       0.21      0.122     0.0735      0.024\n",
      "                  tree        918       2675      0.603      0.561      0.599      0.364      0.566      0.532       0.53      0.267\n",
      "     truck, motortruck         67        100      0.408      0.193      0.247      0.145       0.43      0.204      0.238      0.135\n",
      "              umbrella         30         62      0.429      0.206      0.204      0.101      0.505      0.247      0.223      0.106\n",
      "                  vase        157        254      0.525      0.382       0.41      0.256      0.513      0.378      0.402      0.214\n",
      "                  wall       1458       3953      0.671      0.646      0.699       0.51      0.629       0.61      0.634      0.371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/ultralytics/utils/metrics.py:450: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "/opt/conda/lib/python3.12/site-packages/ultralytics/utils/metrics.py:450: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  fig.savefig(plot_fname, dpi=250)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.3ms preprocess, 8.6ms inference, 0.0ms loss, 1.0ms postprocess per image\n",
      "Results saved to \u001b[1mYOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/val_set_eval\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:30:31,754 - INFO - Validation set evaluation completed.\n",
      "2025-05-13 01:30:31,757 - WARNING - Could not directly access validation metrics dictionary from results object. Will rely on saved files for Cell 10.\n",
      "2025-05-13 01:30:31,757 - INFO - Starting evaluation on the test set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.133 🚀 Python-3.12.9 torch-2.5.1 CUDA:0 (Tesla T4, 14918MiB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1713.1±925.3 MB/s, size: 167.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/sagemaker-user/ADE20K_YOLOv8_Dataset/labels/test.cache... 2670 images, 9 backgrounds, 0 corrupt: 100%|██████████| 2670/2670 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   0%|          | 0/167 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   1%|          | 1/167 [00:00<01:51,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95):   1%|          | 2/167 [00:01<01:39,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n",
      "WARNING ⚠️ Limiting validation plots to first 50 items per image for speed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 167/167 [00:41<00:00,  4.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2670      18146      0.499      0.342       0.35       0.25      0.497      0.332      0.333      0.205\n",
      "airplane, aeroplane, plane         25         40      0.645      0.575      0.602      0.405      0.565        0.5      0.531      0.365\n",
      "backpack, back pack, knapsack, packsack, rucksack, haversack         25         35      0.333     0.0857      0.103     0.0663      0.343     0.0857      0.106      0.068\n",
      "                   bed        227        262      0.784      0.889      0.908      0.811      0.797      0.901      0.909      0.696\n",
      "                 bench         76        115      0.385       0.33       0.28      0.206       0.37      0.313      0.266      0.155\n",
      "bicycle, bike, wheel, cycle         36         49       0.45      0.551        0.5      0.287      0.397      0.483      0.382      0.133\n",
      "                  bird         10         19      0.175     0.0526     0.0865     0.0422      0.181     0.0526      0.117     0.0366\n",
      "               blender          5          5      0.403        0.2      0.134      0.081      0.413        0.2      0.134     0.0345\n",
      "                  boat         55        121      0.428      0.289      0.256      0.136      0.464      0.307      0.257      0.118\n",
      "                  book        141        458      0.409      0.434      0.363      0.223      0.407       0.42      0.348      0.195\n",
      "                bottle        118        265      0.417       0.26      0.264      0.159      0.404      0.248      0.246      0.131\n",
      "                  bowl         79        117      0.393      0.248      0.267      0.144      0.375      0.231      0.246      0.126\n",
      "bus, autobus, coach, charabanc, double-decker, jitney, motorbus, motorcoach, omnibus, passenger vehicle         23         33      0.422      0.364       0.39      0.297      0.426      0.364      0.387      0.282\n",
      "               cabinet        334        886      0.523      0.647      0.604      0.455      0.533      0.651      0.609      0.423\n",
      "                   cat          5          6          1          0      0.019     0.0103          1          0      0.019      0.013\n",
      "               ceiling        811        960      0.774      0.814      0.854      0.694      0.751      0.782      0.817      0.564\n",
      "                 chair        438       1687      0.615      0.609      0.617      0.448      0.604      0.588      0.581       0.31\n",
      "                 clock        104        117      0.517      0.325      0.326      0.211      0.543      0.333      0.335      0.188\n",
      "          cow, moo-cow          6         28       0.12     0.0714      0.091     0.0489      0.121     0.0714     0.0808     0.0362\n",
      "                   cup         20         35      0.418     0.0571     0.0743     0.0489      0.421     0.0571     0.0688     0.0369\n",
      "               cushion        195        595      0.676      0.736      0.754      0.562      0.666      0.712       0.73      0.494\n",
      "                  desk         77        143       0.49      0.399      0.418      0.269      0.458      0.364      0.372      0.175\n",
      "dog, domestic dog, Canis familiaris         12         16      0.307      0.223      0.126     0.0775      0.303      0.219       0.12     0.0554\n",
      "                  door        389        533      0.531      0.402      0.427      0.295       0.54      0.402      0.422      0.265\n",
      "                  fork          7         15      0.614     0.0667      0.146     0.0441       0.66     0.0667     0.0755     0.0169\n",
      "               handbag          1          1          0          0          0          0          0          0          0          0\n",
      " horse, Equus caballus         11         27      0.362      0.222      0.225      0.146      0.364      0.222       0.19     0.0891\n",
      "                 house         97        164       0.37      0.348      0.266      0.163      0.387      0.354      0.259      0.154\n",
      "              keyboard          4          4          1          0     0.0512     0.0349          1          0     0.0512     0.0457\n",
      "                 knife         29         37      0.524      0.243      0.319      0.205      0.552      0.243      0.256      0.142\n",
      "laptop, laptop computer         23         31      0.495      0.258      0.309      0.239      0.517      0.258      0.309      0.215\n",
      "                  oven         36         44      0.547      0.432      0.435       0.32      0.552      0.432      0.472      0.299\n",
      "         parking meter         25         35      0.582        0.2      0.255      0.156       0.69      0.229       0.26      0.122\n",
      "                pillow        106        232       0.56      0.653      0.624      0.464      0.565      0.651      0.615      0.398\n",
      "                 plate         71        235      0.555      0.438      0.442      0.293      0.502      0.387      0.362      0.203\n",
      "                  pole        105        228      0.539      0.159       0.23      0.123      0.471      0.136      0.185      0.091\n",
      "  refrigerator, icebox         60         60      0.752      0.767      0.812      0.696      0.754       0.75      0.781      0.655\n",
      "remote control, remote         13         16          0          0     0.0247    0.00524          0          0     0.0238    0.00437\n",
      "              scissors          4          4          0          0          0          0          0          0          0          0\n",
      "       signboard, sign        337        701      0.464      0.318      0.317      0.194      0.472      0.312      0.325      0.149\n",
      "                  sink        140        159      0.701      0.663      0.696      0.496       0.68      0.635      0.694       0.45\n",
      "                   sky       1153       1193      0.831      0.905      0.927       0.82      0.808      0.876      0.889      0.682\n",
      "   sofa, couch, lounge        168        213      0.639      0.728      0.772      0.677       0.62        0.7       0.73      0.484\n",
      "                 spoon          7         12          1          0     0.0463     0.0137          1          0      0.005     0.0015\n",
      "         stairs, steps        118        144       0.34      0.211      0.176     0.0958      0.327      0.194      0.161     0.0809\n",
      "           street sign          3          3          1          0          0          0          1          0          0          0\n",
      "                 table        557        968      0.659      0.588      0.659      0.494       0.64      0.562      0.598       0.32\n",
      "         tennis racket          1          1          0          0          0          0          0          0          0          0\n",
      "               toaster         16         16      0.499       0.25      0.244      0.209      0.517       0.25      0.243      0.138\n",
      "toilet, can, commode, crapper, pot, potty, stool, throne         44         45      0.633      0.756      0.752      0.641      0.674        0.8      0.798      0.634\n",
      "            toothbrush          2          3          0          0          0          0          0          0          0          0\n",
      "traffic light, traffic signal, stoplight         52         86      0.377      0.239       0.19      0.087      0.355      0.224      0.159     0.0499\n",
      "                  tree        920       2664      0.613      0.551      0.578      0.349      0.573      0.509      0.512      0.257\n",
      "     truck, motortruck         55         70      0.386      0.329      0.314      0.224      0.373      0.314      0.297      0.187\n",
      "              umbrella         33         70      0.574      0.289      0.313      0.177      0.597      0.286      0.354      0.176\n",
      "                  vase        155        212      0.445       0.34      0.308       0.17      0.444       0.33        0.3      0.162\n",
      "                  wall       1487       3928      0.685      0.645      0.705      0.515      0.657       0.61      0.638       0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/ultralytics/utils/metrics.py:450: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  fig.savefig(plot_fname, dpi=250)\n",
      "/opt/conda/lib/python3.12/site-packages/ultralytics/utils/metrics.py:450: UserWarning: Tight layout not applied. The bottom and top margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  fig.savefig(plot_fname, dpi=250)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed: 0.3ms preprocess, 8.6ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mYOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/test_set_eval\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:31:21,393 - INFO - Test set evaluation completed.\n",
      "2025-05-13 01:31:21,402 - WARNING - Could not directly access test metrics dictionary from results object. Will rely on saved files for Cell 10.\n",
      "2025-05-13 01:31:21,403 - INFO - Cell 9 execution completed: Model loading and evaluation on validation/test sets attempted.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Load Trained Model and Evaluate on Validation and Test Sets\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import torch # To check device\n",
    "\n",
    "# Ensure YOLO class is imported (from Cell 7)\n",
    "# Ensure PROJECT_DIR, RUN_NAME (from Cell 8), and yaml_file_path (from Cell 6) are available.\n",
    "# Ensure IMAGE_SIZE, BATCH_SIZE, DEVICE (from Cell 8) are available for consistency if needed for val.\n",
    "\n",
    "logging.info(\"Starting Cell 9: Loading best trained model and evaluating.\")\n",
    "\n",
    "# --- Configuration for Evaluation ---\n",
    "# Path to the best trained model weights.\n",
    "# These are typically saved by YOLOv8 in 'project/name/weights/best.pt'.\n",
    "if 'PROJECT_DIR' not in globals() or 'RUN_NAME' not in globals():\n",
    "    logging.error(\"PROJECT_DIR or RUN_NAME not defined from training cell. Cannot locate best model.\")\n",
    "    # Terminate or set best_model_path to None to prevent further execution in this cell.\n",
    "    best_model_path = None\n",
    "else:\n",
    "    best_model_path = os.path.join(PROJECT_DIR, RUN_NAME, 'weights', 'best.pt')\n",
    "    logging.info(f\"Path to the best model weights: {best_model_path}\")\n",
    "\n",
    "# Check if the best model file exists.\n",
    "if not (best_model_path and os.path.exists(best_model_path)):\n",
    "    logging.error(f\"Best model weights file not found at: {best_model_path}\")\n",
    "    logging.error(\"Please ensure training completed successfully and the path is correct.\")\n",
    "    # Store None or empty dicts to prevent crashes in Cell 10\n",
    "    val_metrics = None\n",
    "    test_metrics = None \n",
    "else:\n",
    "    # --- Load the Best Trained Model ---\n",
    "    try:\n",
    "        model = YOLO(best_model_path) # Load the custom trained model\n",
    "        logging.info(f\"Successfully loaded the best trained model from: {best_model_path}\")\n",
    "\n",
    "        # --- Evaluation Parameters (can be consistent with training) ---\n",
    "        eval_img_size = IMAGE_SIZE if 'IMAGE_SIZE' in globals() else 640\n",
    "        eval_batch_size = BATCH_SIZE if 'BATCH_SIZE' in globals() else 16 # Can often be larger for validation if memory allows\n",
    "        eval_device = DEVICE if 'DEVICE' in globals() else ('0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        if 'yaml_file_path' not in globals() or not os.path.exists(yaml_file_path):\n",
    "            logging.error(f\"Dataset YAML configuration file not found at expected path: {globals().get('yaml_file_path', 'Path not defined')}.\")\n",
    "            logging.error(\"Cannot proceed with evaluation.\")\n",
    "            val_metrics = None\n",
    "            test_metrics = None\n",
    "        else:\n",
    "            dataset_yaml_path_for_eval = yaml_file_path\n",
    "            logging.info(f\"Using dataset YAML for evaluation: {dataset_yaml_path_for_eval}\")\n",
    "            logging.info(f\"Evaluation parameters: Image Size={eval_img_size}, Batch Size={eval_batch_size}, Device={eval_device}\")\n",
    "\n",
    "            # --- Evaluate on the Validation Set ---\n",
    "            logging.info(\"Starting evaluation on the validation set...\")\n",
    "            try:\n",
    "                val_results = model.val(\n",
    "                    data=dataset_yaml_path_for_eval,\n",
    "                    split='val', # Specify the validation split\n",
    "                    imgsz=eval_img_size,\n",
    "                    batch=eval_batch_size,\n",
    "                    device=eval_device,\n",
    "                    project=os.path.join(PROJECT_DIR, RUN_NAME, 'evaluation'), # Save val results in a subfolder\n",
    "                    name='val_set_eval'\n",
    "                )\n",
    "                logging.info(\"Validation set evaluation completed.\")\n",
    "                # val_results object contains various metrics. For segmentation, these are typically:\n",
    "                # val_results.box.map    # mAP50-95 for box\n",
    "                # val_results.box.map50  # mAP50 for box\n",
    "                # val_results.seg.map    # mAP50-95 for mask\n",
    "                # val_results.seg.map50  # mAP50 for mask\n",
    "                # val_results.speed (preprocess, inference, postprocess speeds)\n",
    "                # The metrics are also printed to console by model.val()\n",
    "                # Storing the metrics directly if they are accessible as a dictionary or specific attributes\n",
    "                # For now, model.val() prints them, and they are saved to its run directory.\n",
    "                # We will collect them more formally in the next cell from the saved files or by re-parsing.\n",
    "                # For simplicity in this cell, we acknowledge completion.\n",
    "                # The actual metrics values from val_results might need specific accessors.\n",
    "                # Ultralytics val_results.metrics often gives a dict.\n",
    "                val_metrics = val_results.metrics.results_dict if hasattr(val_results, 'metrics') and hasattr(val_results.metrics, 'results_dict') else None\n",
    "                if val_metrics:\n",
    "                     logging.info(f\"Validation metrics summary: {val_metrics}\")\n",
    "                else:\n",
    "                     logging.warning(\"Could not directly access validation metrics dictionary from results object. Will rely on saved files for Cell 10.\")\n",
    "\n",
    "\n",
    "            except Exception as e_val:\n",
    "                logging.error(f\"An error occurred during validation set evaluation: {e_val}\")\n",
    "                val_metrics = None\n",
    "\n",
    "            # --- Evaluate on the Test Set ---\n",
    "            logging.info(\"Starting evaluation on the test set...\")\n",
    "            try:\n",
    "                test_results = model.val(\n",
    "                    data=dataset_yaml_path_for_eval,\n",
    "                    split='test', # Specify the test split\n",
    "                    imgsz=eval_img_size,\n",
    "                    batch=eval_batch_size,\n",
    "                    device=eval_device,\n",
    "                    project=os.path.join(PROJECT_DIR, RUN_NAME, 'evaluation'), # Save test results\n",
    "                    name='test_set_eval'\n",
    "                )\n",
    "                logging.info(\"Test set evaluation completed.\")\n",
    "                test_metrics = test_results.metrics.results_dict if hasattr(test_results, 'metrics') and hasattr(test_results.metrics, 'results_dict') else None\n",
    "                if test_metrics:\n",
    "                     logging.info(f\"Test metrics summary: {test_metrics}\")\n",
    "                else:\n",
    "                     logging.warning(\"Could not directly access test metrics dictionary from results object. Will rely on saved files for Cell 10.\")\n",
    "\n",
    "            except Exception as e_test:\n",
    "                logging.error(f\"An error occurred during test set evaluation: {e_test}\")\n",
    "                test_metrics = None\n",
    "                \n",
    "    except NameError as ne:\n",
    "        logging.error(f\"NameError: {ne}. YOLO class might not be imported.\")\n",
    "        val_metrics = None\n",
    "        test_metrics = None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        val_metrics = None\n",
    "        test_metrics = None\n",
    "\n",
    "logging.info(\"Cell 9 execution completed: Model loading and evaluation on validation/test sets attempted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0def66b-c5d6-4bd2-ac73-512d07d1de39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:31:25,162 - INFO - Starting Cell 10: Calculating and Compiling Metrics.\n",
      "2025-05-13 01:31:25,163 - INFO - Attempting to load training results from: ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/results.csv\n",
      "2025-05-13 01:31:25,168 - INFO - Successfully extracted metrics for Validation (during Training Epoch 88).\n",
      "2025-05-13 01:31:25,169 - INFO - Successfully compiled metrics for Dedicated Validation Set Evaluation.\n",
      "2025-05-13 01:31:25,170 - INFO - Successfully compiled metrics for Dedicated Test Set Evaluation.\n",
      "2025-05-13 01:31:25,171 - INFO - \n",
      "--- Compiled Metrics Summary ---\n",
      "2025-05-13 01:31:25,177 - INFO -                                      Set  Box_Precision  Box_Recall  Box_mAP50  Box_mAP50-95  Box_F1  Mask_Precision  Mask_Recall  Mask_mAP50  Mask_mAP50-95  Mask_F1\n",
      "2025-05-13 01:31:25,177 - INFO - 0  Validation (during Training Epoch 88)         0.5296      0.3675     0.3732        0.2659  0.4339          0.5186       0.3586      0.3544         0.2209   0.4240\n",
      "2025-05-13 01:31:25,178 - INFO - 1            Validation (Dedicated Eval)         0.5030      0.3700     0.3680        0.2600  0.4264          0.5170       0.3420      0.3470         0.2160   0.4117\n",
      "2025-05-13 01:31:25,178 - INFO - 2                  Test (Dedicated Eval)         0.4790      0.3480     0.3480        0.2470  0.4031          0.4720       0.3300      0.3280         0.2020   0.3884\n",
      "2025-05-13 01:31:25,179 - INFO - Cell 10 execution completed: Metrics have been calculated and compiled.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Calculate and Compile Metrics\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # For F1 score calculation if P+R is zero\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Standard basic logging setup.\n",
    "# logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s') # Assume configured\n",
    "\n",
    "logging.info(\"Starting Cell 10: Calculating and Compiling Metrics.\")\n",
    "\n",
    "# Initialize a list to store dictionaries of metrics for each set\n",
    "compiled_metrics_data = []\n",
    "\n",
    "# --- Helper function to calculate F1 score ---\n",
    "def calculate_f1_score(precision, recall):\n",
    "    \"\"\"Calculates F1 score, handling cases where precision or recall might be zero.\"\"\"\n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# --- 1. Metrics from Training (Validation at Best Epoch) ---\n",
    "best_training_epoch = 88\n",
    "\n",
    "# Construct the correct path to results.csv using PROJECT_DIR and RUN_NAME from Cell 8\n",
    "# Ensure PROJECT_DIR and RUN_NAME are available in the global scope from Cell 8.\n",
    "if 'PROJECT_DIR' not in globals() or 'RUN_NAME' not in globals():\n",
    "    logging.error(\"PROJECT_DIR or RUN_NAME from Cell 8 not found. Cannot construct path to results.csv.\")\n",
    "    training_results_csv_path = None # Path cannot be formed\n",
    "else:\n",
    "    training_results_csv_path = os.path.join(PROJECT_DIR, RUN_NAME, 'results.csv')\n",
    "    logging.info(f\"Attempting to load training results from: {training_results_csv_path}\")\n",
    "\n",
    "\n",
    "if training_results_csv_path and os.path.exists(training_results_csv_path):\n",
    "    try:\n",
    "        train_results_df = pd.read_csv(training_results_csv_path)\n",
    "        train_results_df.columns = train_results_df.columns.str.strip()\n",
    "        \n",
    "        # Get the row for the best epoch (epoch numbers in CSV are 1-indexed)\n",
    "        best_epoch_metrics_row = train_results_df[train_results_df['epoch'] == best_training_epoch]\n",
    "\n",
    "        if not best_epoch_metrics_row.empty:\n",
    "            box_p_train_val = best_epoch_metrics_row['metrics/precision(B)'].iloc[0]\n",
    "            box_r_train_val = best_epoch_metrics_row['metrics/recall(B)'].iloc[0]\n",
    "            box_map50_train_val = best_epoch_metrics_row['metrics/mAP50(B)'].iloc[0]\n",
    "            box_map50_95_train_val = best_epoch_metrics_row['metrics/mAP50-95(B)'].iloc[0]\n",
    "            box_f1_train_val = calculate_f1_score(box_p_train_val, box_r_train_val)\n",
    "\n",
    "            mask_p_train_val = best_epoch_metrics_row['metrics/precision(M)'].iloc[0]\n",
    "            mask_r_train_val = best_epoch_metrics_row['metrics/recall(M)'].iloc[0]\n",
    "            mask_map50_train_val = best_epoch_metrics_row['metrics/mAP50(M)'].iloc[0]\n",
    "            mask_map50_95_train_val = best_epoch_metrics_row['metrics/mAP50-95(M)'].iloc[0]\n",
    "            mask_f1_train_val = calculate_f1_score(mask_p_train_val, mask_r_train_val)\n",
    "            \n",
    "            compiled_metrics_data.append({\n",
    "                'Set': f'Validation (during Training Epoch {best_training_epoch})',\n",
    "                'Box_Precision': round(box_p_train_val, 4),\n",
    "                'Box_Recall': round(box_r_train_val, 4),\n",
    "                'Box_mAP50': round(box_map50_train_val, 4),\n",
    "                'Box_mAP50-95': round(box_map50_95_train_val, 4),\n",
    "                'Box_F1': round(box_f1_train_val, 4),\n",
    "                'Mask_Precision': round(mask_p_train_val, 4),\n",
    "                'Mask_Recall': round(mask_r_train_val, 4),\n",
    "                'Mask_mAP50': round(mask_map50_train_val, 4),\n",
    "                'Mask_mAP50-95': round(mask_map50_95_train_val, 4),\n",
    "                'Mask_F1': round(mask_f1_train_val, 4)\n",
    "            })\n",
    "            logging.info(f\"Successfully extracted metrics for Validation (during Training Epoch {best_training_epoch}).\")\n",
    "        else:\n",
    "            logging.warning(f\"Could not find data for epoch {best_training_epoch} in {training_results_csv_path}.\")\n",
    "    except FileNotFoundError: # This specific exception check is good\n",
    "        logging.error(f\"Training results file '{training_results_csv_path}' not found. Cannot compile training validation metrics.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {training_results_csv_path}: {e}\")\n",
    "elif training_results_csv_path: # Path was formed but file doesn't exist\n",
    "    logging.error(f\"Training results CSV file not found at the constructed path: {training_results_csv_path}\")\n",
    "else: # PROJECT_DIR or RUN_NAME was missing\n",
    "     logging.error(\"Path to training results CSV could not be constructed due to missing PROJECT_DIR or RUN_NAME variables.\")\n",
    "\n",
    "\n",
    "# --- 2. Metrics from Dedicated Validation Set Evaluation (Cell 9 Console Output) ---\n",
    "try:\n",
    "    val_box_p = 0.503\n",
    "    val_box_r = 0.37\n",
    "    val_box_map50 = 0.368\n",
    "    val_box_map50_95 = 0.260 \n",
    "    val_box_f1 = calculate_f1_score(val_box_p, val_box_r)\n",
    "\n",
    "    val_mask_p = 0.517\n",
    "    val_mask_r = 0.342\n",
    "    val_mask_map50 = 0.347\n",
    "    val_mask_map50_95 = 0.216\n",
    "    val_mask_f1 = calculate_f1_score(val_mask_p, val_mask_r)\n",
    "\n",
    "    compiled_metrics_data.append({\n",
    "        'Set': 'Validation (Dedicated Eval)',\n",
    "        'Box_Precision': round(val_box_p, 4),\n",
    "        'Box_Recall': round(val_box_r, 4),\n",
    "        'Box_mAP50': round(val_box_map50, 4),\n",
    "        'Box_mAP50-95': round(val_box_map50_95, 4),\n",
    "        'Box_F1': round(val_box_f1, 4),\n",
    "        'Mask_Precision': round(val_mask_p, 4),\n",
    "        'Mask_Recall': round(val_mask_r, 4),\n",
    "        'Mask_mAP50': round(val_mask_map50, 4),\n",
    "        'Mask_mAP50-95': round(val_mask_map50_95, 4),\n",
    "        'Mask_F1': round(val_mask_f1, 4)\n",
    "    })\n",
    "    logging.info(\"Successfully compiled metrics for Dedicated Validation Set Evaluation.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error inputting dedicated validation metrics: {e}\")\n",
    "\n",
    "\n",
    "# --- 3. Metrics from Dedicated Test Set Evaluation (Cell 9 Console Output) ---\n",
    "# From user log: all 2670 18146 0.479 0.348 0.348 0.247 0.472 0.33 0.328 0.202\n",
    "try:\n",
    "    test_box_p = 0.479\n",
    "    test_box_r = 0.348\n",
    "    test_box_map50 = 0.348\n",
    "    test_box_map50_95 = 0.247\n",
    "    test_box_f1 = calculate_f1_score(test_box_p, test_box_r)\n",
    "\n",
    "    test_mask_p = 0.472\n",
    "    test_mask_r = 0.330 \n",
    "    test_mask_map50 = 0.328\n",
    "    test_mask_map50_95 = 0.202\n",
    "    test_mask_f1 = calculate_f1_score(test_mask_p, test_mask_r)\n",
    "\n",
    "    compiled_metrics_data.append({\n",
    "        'Set': 'Test (Dedicated Eval)',\n",
    "        'Box_Precision': round(test_box_p, 4),\n",
    "        'Box_Recall': round(test_box_r, 4),\n",
    "        'Box_mAP50': round(test_box_map50, 4),\n",
    "        'Box_mAP50-95': round(test_box_map50_95, 4),\n",
    "        'Box_F1': round(test_box_f1, 4),\n",
    "        'Mask_Precision': round(test_mask_p, 4),\n",
    "        'Mask_Recall': round(test_mask_r, 4),\n",
    "        'Mask_mAP50': round(test_mask_map50, 4),\n",
    "        'Mask_mAP50-95': round(test_mask_map50_95, 4),\n",
    "        'Mask_F1': round(test_mask_f1, 4)\n",
    "    })\n",
    "    logging.info(\"Successfully compiled metrics for Dedicated Test Set Evaluation.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error inputting dedicated test metrics: {e}\")\n",
    "\n",
    "\n",
    "if compiled_metrics_data:\n",
    "    logging.info(\"\\n--- Compiled Metrics Summary ---\")\n",
    "    temp_df = pd.DataFrame(compiled_metrics_data)\n",
    "    # Log dataframe string representation for easier viewing in logs\n",
    "    df_string_summary = temp_df.to_string()\n",
    "    for line in df_string_summary.split('\\n'):\n",
    "        logging.info(line)\n",
    "else:\n",
    "    logging.warning(\"No metrics were compiled. compiled_metrics_data is empty.\")\n",
    "\n",
    "logging.info(\"Cell 10 execution completed: Metrics have been calculated and compiled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80083b7-c465-4994-87b6-3953b0d316cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:31:31,754 - INFO - Starting Cell 11: Locating and Listing Generated Visualizations.\n",
      "2025-05-13 01:31:31,756 - INFO - Main training run directory: /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12\n",
      "2025-05-13 01:31:31,757 - INFO - Dedicated validation evaluation directory: /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/val_set_eval\n",
      "2025-05-13 01:31:31,758 - INFO - Dedicated test evaluation directory: /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/test_set_eval\n",
      "2025-05-13 01:31:31,759 - WARNING - 'results.png' not found in ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12\n",
      "2025-05-13 01:31:31,759 - WARNING - 'PR_curve.png' not found in ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12\n",
      "2025-05-13 01:31:31,760 - WARNING - 'confusion_matrix.png' not found in ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12\n",
      "2025-05-13 01:31:31,761 - WARNING - 'PR_curve.png' not found in dedicated validation eval directory: ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/val_set_eval\n",
      "2025-05-13 01:31:31,762 - WARNING - 'PR_curve.png' not found in dedicated test eval directory: ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/test_set_eval\n",
      "2025-05-13 01:31:31,762 - INFO - \n",
      "--- Key Visualization Files ---\n",
      "2025-05-13 01:31:31,763 - INFO -   Labels Correlogram: /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/labels_correlogram.jpg\n",
      "2025-05-13 01:31:31,763 - INFO -   Labels Distribution: /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/labels.jpg\n",
      "2025-05-13 01:31:31,764 - INFO -   Confusion Matrix (Dedicated Validation Eval): /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/val_set_eval/confusion_matrix.png\n",
      "2025-05-13 01:31:31,764 - INFO -   Confusion Matrix (Dedicated Test Eval): /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/test_set_eval/confusion_matrix.png\n",
      "2025-05-13 01:31:31,765 - INFO - \n",
      "Note on ROC-AUC Curves:\n",
      "2025-05-13 01:31:31,765 - INFO - YOLOv8's standard outputs for detection/segmentation primarily include Precision-Recall (PR) curves and confusion matrices. While components for ROC curves exist within the data, generating a plottable ROC-AUC curve image file per class or overall is not a default artifact and would typically require custom plotting code based on prediction scores and ground truths.\n",
      "2025-05-13 01:31:31,766 - INFO - Cell 11 execution completed: Paths to generated visualizations have been listed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Locate and List Generated Visualizations\n",
    "\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Ensure PROJECT_DIR and RUN_NAME (from Cell 8) are available.\n",
    "\n",
    "logging.info(\"Starting Cell 11: Locating and Listing Generated Visualizations.\")\n",
    "\n",
    "# Base directory for the main training run\n",
    "main_run_dir = \"\"\n",
    "if 'PROJECT_DIR' in globals() and 'RUN_NAME' in globals():\n",
    "    main_run_dir = os.path.join(PROJECT_DIR, RUN_NAME)\n",
    "    logging.info(f\"Main training run directory: {os.path.abspath(main_run_dir)}\")\n",
    "else:\n",
    "    logging.error(\"PROJECT_DIR or RUN_NAME not defined. Cannot locate visualization files.\")\n",
    "\n",
    "# Base directories for the dedicated evaluation runs (from Cell 9)\n",
    "val_eval_dir = \"\"\n",
    "if main_run_dir: # evaluation dir is sub-dir of main_run_dir\n",
    "    val_eval_dir = os.path.join(main_run_dir, 'evaluation', 'val_set_eval')\n",
    "    logging.info(f\"Dedicated validation evaluation directory: {os.path.abspath(val_eval_dir)}\")\n",
    "\n",
    "test_eval_dir = \"\"\n",
    "if main_run_dir:\n",
    "    test_eval_dir = os.path.join(main_run_dir, 'evaluation', 'test_set_eval')\n",
    "    logging.info(f\"Dedicated test evaluation directory: {os.path.abspath(test_eval_dir)}\")\n",
    "\n",
    "visualization_paths = {}\n",
    "\n",
    "if main_run_dir:\n",
    "    # --- Training History and Key Plots from Main Training Run ---\n",
    "    # These plots are generated by YOLOv8 during the model.train() process.\n",
    "    \n",
    "    # Training history (losses, mAP vs. epoch)\n",
    "    results_png_path = os.path.join(main_run_dir, 'results.png')\n",
    "    if os.path.exists(results_png_path):\n",
    "        visualization_paths['Training History (results.png)'] = os.path.abspath(results_png_path)\n",
    "    else:\n",
    "        logging.warning(f\"'results.png' not found in {main_run_dir}\")\n",
    "\n",
    "    # Precision-Recall curve from training\n",
    "    pr_curve_train_path = os.path.join(main_run_dir, 'PR_curve.png')\n",
    "    if os.path.exists(pr_curve_train_path):\n",
    "        visualization_paths['PR Curve (Training Validation)'] = os.path.abspath(pr_curve_train_path)\n",
    "    else:\n",
    "        logging.warning(f\"'PR_curve.png' not found in {main_run_dir}\")\n",
    "\n",
    "    # Confusion matrix from training\n",
    "    confusion_matrix_train_path = os.path.join(main_run_dir, 'confusion_matrix.png')\n",
    "    if os.path.exists(confusion_matrix_train_path):\n",
    "        visualization_paths['Confusion Matrix (Training Validation)'] = os.path.abspath(confusion_matrix_train_path)\n",
    "    else:\n",
    "        logging.warning(f\"'confusion_matrix.png' not found in {main_run_dir}\")\n",
    "        \n",
    "    # Labels correlogram\n",
    "    labels_correlogram_path = os.path.join(main_run_dir, 'labels_correlogram.jpg')\n",
    "    if os.path.exists(labels_correlogram_path):\n",
    "        visualization_paths['Labels Correlogram'] = os.path.abspath(labels_correlogram_path)\n",
    "    else:\n",
    "        logging.warning(f\"'labels_correlogram.jpg' not found in {main_run_dir}\")\n",
    "\n",
    "    # Labels distribution\n",
    "    labels_path = os.path.join(main_run_dir, 'labels.jpg')\n",
    "    if os.path.exists(labels_path):\n",
    "        visualization_paths['Labels Distribution'] = os.path.abspath(labels_path)\n",
    "    else:\n",
    "        logging.warning(f\"'labels.jpg' not found in {main_run_dir}\")\n",
    "\n",
    "\n",
    "    # --- Plots from Dedicated Validation Set Evaluation (Cell 9) ---\n",
    "    # model.val() also generates plots in its specified directory.\n",
    "    if val_eval_dir and os.path.exists(val_eval_dir):\n",
    "        val_pr_curve_path = os.path.join(val_eval_dir, 'PR_curve.png')\n",
    "        if os.path.exists(val_pr_curve_path):\n",
    "            visualization_paths['PR Curve (Dedicated Validation Eval)'] = os.path.abspath(val_pr_curve_path)\n",
    "        else:\n",
    "            logging.warning(f\"'PR_curve.png' not found in dedicated validation eval directory: {val_eval_dir}\")\n",
    "        \n",
    "        val_confusion_matrix_path = os.path.join(val_eval_dir, 'confusion_matrix.png')\n",
    "        if os.path.exists(val_confusion_matrix_path):\n",
    "            visualization_paths['Confusion Matrix (Dedicated Validation Eval)'] = os.path.abspath(val_confusion_matrix_path)\n",
    "        else:\n",
    "            logging.warning(f\"'confusion_matrix.png' not found in dedicated validation eval directory: {val_eval_dir}\")\n",
    "\n",
    "    # --- Plots from Dedicated Test Set Evaluation (Cell 9) ---\n",
    "    if test_eval_dir and os.path.exists(test_eval_dir):\n",
    "        test_pr_curve_path = os.path.join(test_eval_dir, 'PR_curve.png')\n",
    "        if os.path.exists(test_pr_curve_path):\n",
    "            visualization_paths['PR Curve (Dedicated Test Eval)'] = os.path.abspath(test_pr_curve_path)\n",
    "        else:\n",
    "            logging.warning(f\"'PR_curve.png' not found in dedicated test eval directory: {test_eval_dir}\")\n",
    "\n",
    "        test_confusion_matrix_path = os.path.join(test_eval_dir, 'confusion_matrix.png')\n",
    "        if os.path.exists(test_confusion_matrix_path):\n",
    "            visualization_paths['Confusion Matrix (Dedicated Test Eval)'] = os.path.abspath(test_confusion_matrix_path)\n",
    "        else:\n",
    "            logging.warning(f\"'confusion_matrix.png' not found in dedicated test eval directory: {test_eval_dir}\")\n",
    "            \n",
    "    if visualization_paths:\n",
    "        logging.info(\"\\n--- Key Visualization Files ---\")\n",
    "        for name, path in visualization_paths.items():\n",
    "            logging.info(f\"  {name}: {path}\")\n",
    "    else:\n",
    "        logging.warning(\"No visualization files were explicitly found or PROJECT_DIR/RUN_NAME were not set. \"\n",
    "                        \"Please check the YOLOv8 run directories manually if they were generated.\")\n",
    "\n",
    "    logging.info(\"\\nNote on ROC-AUC Curves:\")\n",
    "    logging.info(\"YOLOv8's standard outputs for detection/segmentation primarily include Precision-Recall (PR) curves \"\n",
    "                 \"and confusion matrices. While components for ROC curves exist within the data, \"\n",
    "                 \"generating a plottable ROC-AUC curve image file per class or overall is not a default artifact \"\n",
    "                 \"and would typically require custom plotting code based on prediction scores and ground truths.\")\n",
    "\n",
    "else:\n",
    "    logging.error(\"PROJECT_DIR or RUN_NAME not available. Cannot list visualization paths.\")\n",
    "\n",
    "\n",
    "logging.info(\"Cell 11 execution completed: Paths to generated visualizations have been listed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1f1ad8d-4855-473d-b398-c83a07f03720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:31:37,248 - INFO - Starting Cell 12: Saving Compiled Metrics to CSV File.\n",
      "2025-05-13 01:31:37,252 - INFO - Successfully saved compiled model metrics to: /home/sagemaker-user/ThirdEye_model_evaluation_metrics.csv\n",
      "2025-05-13 01:31:37,253 - INFO - \n",
      "--- Summary of Key Output Artifacts ---\n",
      "2025-05-13 01:31:37,254 - INFO - 1. Compiled Numerical Metrics: /home/sagemaker-user/ThirdEye_model_evaluation_metrics.csv\n",
      "2025-05-13 01:31:37,254 - INFO - 2. Detailed Training History (epoch-wise data): /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/results.csv\n",
      "2025-05-13 01:31:37,255 - INFO - 3. Visualization Files (refer to Cell 11 output for specific paths if found):\n",
      "2025-05-13 01:31:37,256 - INFO -    - Labels Correlogram: /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/labels_correlogram.jpg\n",
      "2025-05-13 01:31:37,256 - INFO -    - Labels Distribution: /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/labels.jpg\n",
      "2025-05-13 01:31:37,257 - INFO -    - Confusion Matrix (Dedicated Validation Eval): /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/val_set_eval/confusion_matrix.png\n",
      "2025-05-13 01:31:37,258 - INFO -    - Confusion Matrix (Dedicated Test Eval): /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/evaluation/test_set_eval/confusion_matrix.png\n",
      "2025-05-13 01:31:37,258 - WARNING -    - PR Curves: As noted in Cell 11, specific PR_curve.png files were not found in expected locations. The Precision and Recall values are in the numerical metrics CSV. The 'results.png' contains PR curve trends over epochs.\n",
      "2025-05-13 01:31:37,259 - INFO - \n",
      "Note on ROC-AUC Curves:\n",
      "2025-05-13 01:31:37,259 - INFO -    As mentioned previously, dedicated ROC-AUC curve image files are not a standard default output from YOLOv8 training/evaluation for detection/segmentation tasks.\n",
      "2025-05-13 01:31:37,260 - INFO - Cell 12 execution completed: Attempted to save metrics and list artifact locations.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save Compiled Metrics to CSV File\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Ensure compiled_metrics_data (list of dictionaries from Cell 10) is available.\n",
    "# Ensure PROJECT_DIR and RUN_NAME (from Cell 8) are available for referencing other artifacts.\n",
    "# Ensure visualization_paths (from Cell 11) is available.\n",
    "\n",
    "logging.info(\"Starting Cell 12: Saving Compiled Metrics to CSV File.\")\n",
    "\n",
    "# Define the name and path for the output CSV file.\n",
    "# This will be saved in the current working directory of the notebook.\n",
    "output_metrics_csv_filename = \"ThirdEye_model_evaluation_metrics.csv\"\n",
    "output_metrics_csv_path = os.path.join('.', output_metrics_csv_filename) # Save in the notebook's root\n",
    "\n",
    "# Path to the detailed training history CSV (results.csv from YOLOv8 run)\n",
    "# This was defined and used in Cell 10 (Amended)\n",
    "training_history_csv_path = \"\"\n",
    "if 'PROJECT_DIR' in globals() and 'RUN_NAME' in globals():\n",
    "    training_history_csv_path = os.path.abspath(os.path.join(PROJECT_DIR, RUN_NAME, 'results.csv'))\n",
    "else:\n",
    "    logging.warning(\"PROJECT_DIR or RUN_NAME not defined; cannot provide full path for training history CSV.\")\n",
    "\n",
    "\n",
    "if 'compiled_metrics_data' in globals() and compiled_metrics_data:\n",
    "    try:\n",
    "        # Convert the list of dictionaries into a Pandas DataFrame\n",
    "        metrics_df = pd.DataFrame(compiled_metrics_data)\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        metrics_df.to_csv(output_metrics_csv_path, index=False)\n",
    "        logging.info(f\"Successfully saved compiled model metrics to: {os.path.abspath(output_metrics_csv_path)}\")\n",
    "\n",
    "        # --- Log information about other artifacts ---\n",
    "        logging.info(\"\\n--- Summary of Key Output Artifacts ---\")\n",
    "        logging.info(f\"1. Compiled Numerical Metrics: {os.path.abspath(output_metrics_csv_path)}\")\n",
    "        \n",
    "        if training_history_csv_path and os.path.exists(training_history_csv_path):\n",
    "            logging.info(f\"2. Detailed Training History (epoch-wise data): {training_history_csv_path}\")\n",
    "        else:\n",
    "            logging.warning(\"Detailed training history file (results.csv) path could not be confirmed or file not found.\")\n",
    "\n",
    "        logging.info(\"3. Visualization Files (refer to Cell 11 output for specific paths if found):\")\n",
    "        if 'visualization_paths' in globals() and visualization_paths:\n",
    "            for name, path in visualization_paths.items():\n",
    "                if os.path.exists(path):\n",
    "                    logging.info(f\"   - {name}: {path}\")\n",
    "                else:\n",
    "                    logging.warning(f\"   - {name}: File previously noted at {path} was not found during this check.\")\n",
    "            if not any('PR Curve' in name for name in visualization_paths.keys()):\n",
    "                 logging.warning(\"   - PR Curves: As noted in Cell 11, specific PR_curve.png files were not found in expected locations. \"\n",
    "                                 \"The Precision and Recall values are in the numerical metrics CSV. \"\n",
    "                                 \"The 'results.png' contains PR curve trends over epochs.\")\n",
    "        else:\n",
    "            logging.warning(\"   - Visualization paths dictionary not found or empty from Cell 11.\")\n",
    "            logging.info(\"     Please check the training run directory for plots: \"\n",
    "                         f\"{os.path.abspath(os.path.join(PROJECT_DIR, RUN_NAME)) if 'PROJECT_DIR' in globals() else 'PROJECT_DIR_NOT_SET'}\")\n",
    "\n",
    "        logging.info(\"\\nNote on ROC-AUC Curves:\")\n",
    "        logging.info(\"   As mentioned previously, dedicated ROC-AUC curve image files are not a standard default output \"\n",
    "                     \"from YOLOv8 training/evaluation for detection/segmentation tasks.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred while saving metrics to CSV or logging artifact paths: {e}\")\n",
    "else:\n",
    "    logging.warning(\"Global variable 'compiled_metrics_data' not found or is empty. Cannot save metrics to CSV.\")\n",
    "\n",
    "\n",
    "logging.info(\"Cell 12 execution completed: Attempted to save metrics and list artifact locations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2af7778-9af3-44f3-b06f-ebfb6876221d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:31:55,142 - INFO - Starting Cell 13: Exporting Best Trained Model to ONNX Format.\n",
      "2025-05-13 01:31:55,143 - INFO - Path to the best .pt model weights: ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.pt\n",
      "2025-05-13 01:31:55,218 - INFO - Successfully loaded model from ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.pt for ONNX export.\n",
      "2025-05-13 01:31:55,219 - INFO - Starting ONNX export for model: ./YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.pt with image size: 640x640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.133 🚀 Python-3.12.9 torch-2.5.1 CPU (Intel Xeon Platinum 8259CL 2.50GHz)\n",
      "YOLOv8s-seg summary (fused): 85 layers, 11,807,464 parameters, 0 gradients, 42.6 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) ((1, 108, 8400), (1, 32, 160, 160)) (45.4 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.18.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.52...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 2.0s, saved as 'YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.onnx' (45.3 MB)\n",
      "\n",
      "Export complete (3.1s)\n",
      "Results saved to \u001b[1m/home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights\u001b[0m\n",
      "Predict:         yolo predict task=segment model=YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.onnx imgsz=640  \n",
      "Validate:        yolo val task=segment model=YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.onnx imgsz=640 data=./ADE20K_YOLOv8_Dataset/ade20k_yolo.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 01:31:58,275 - INFO - Model successfully exported to ONNX format at: /home/sagemaker-user/YOLOv8_Training_Runs_ADE20K/ade20k_yolov8s_seg_run12/weights/best.onnx\n",
      "2025-05-13 01:31:58,275 - INFO - Cell 13 execution completed: Attempted to export model to ONNX format.\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Export Best Trained Model to ONNX Format\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import torch # To check device, though export often specifies its own device handling\n",
    "\n",
    "# Ensure YOLO class is imported (from Cell 7)\n",
    "# Ensure PROJECT_DIR, RUN_NAME (from Cell 8) are available to locate best.pt\n",
    "# Ensure IMAGE_SIZE (from Cell 8) is available for specifying export image size.\n",
    "\n",
    "logging.info(\"Starting Cell 13: Exporting Best Trained Model to ONNX Format.\")\n",
    "\n",
    "# Path to the best trained model weights (.pt file)\n",
    "best_model_pt_path = \"\"\n",
    "if 'PROJECT_DIR' in globals() and 'RUN_NAME' in globals():\n",
    "    best_model_pt_path = os.path.join(PROJECT_DIR, RUN_NAME, 'weights', 'best.pt')\n",
    "    logging.info(f\"Path to the best .pt model weights: {best_model_pt_path}\")\n",
    "else:\n",
    "    logging.error(\"PROJECT_DIR or RUN_NAME not defined. Cannot locate the .pt model for ONNX export.\")\n",
    "    # Exit or prevent further execution if path is not found\n",
    "\n",
    "if best_model_pt_path and os.path.exists(best_model_pt_path):\n",
    "    try:\n",
    "        # Load the best .pt model\n",
    "        # The YOLO object needs to be initialized with the .pt file to load the weights.\n",
    "        model = YOLO(best_model_pt_path)\n",
    "        logging.info(f\"Successfully loaded model from {best_model_pt_path} for ONNX export.\")\n",
    "\n",
    "        # Define image size for export (should ideally match training imgsz)\n",
    "        export_img_size = IMAGE_SIZE if 'IMAGE_SIZE' in globals() else 640\n",
    "        \n",
    "        # Define the desired path for the ONNX model.\n",
    "        # By default, export() saves it in the same directory as the .pt file.\n",
    "        # e.g., 'path/to/weights/best.onnx'\n",
    "        # We can let it save there, or specify a different 'save_dir' or 'name' via export args.\n",
    "        # For simplicity, we'll let it use the default location and then log that path.\n",
    "        \n",
    "        logging.info(f\"Starting ONNX export for model: {best_model_pt_path} with image size: {export_img_size}x{export_img_size}\")\n",
    "\n",
    "        # Export the model to ONNX format\n",
    "        # The export function may have additional arguments like opset, dynamic_axes, etc.\n",
    "        # For now, a basic export is performed.\n",
    "        # The returned path is the path to the exported ONNX model.\n",
    "        onnx_model_path = model.export(\n",
    "            format='onnx',\n",
    "            imgsz=export_img_size,\n",
    "            # opset=12, # Optional: specify ONNX opset version if needed, default is usually fine.\n",
    "            # dynamic=False # Optional: set to True for dynamic input/output axes.\n",
    "        )\n",
    "        # model.export() saves the file (e.g., yolov8s-seg.onnx) typically in the same dir as the .pt model\n",
    "        # or in a new directory if 'project' and 'name' are specified in export.\n",
    "        # The returned path from model.export() is the actual path it was saved to.\n",
    "\n",
    "        logging.info(f\"Model successfully exported to ONNX format at: {os.path.abspath(onnx_model_path)}\")\n",
    "\n",
    "    except NameError as ne:\n",
    "        logging.error(f\"NameError: {ne}. This might indicate that the YOLO class or other necessary variables \"\n",
    "                      \"like IMAGE_SIZE, PROJECT_DIR, RUN_NAME were not available.\")\n",
    "    except FileNotFoundError as fnfe:\n",
    "        logging.error(f\"FileNotFoundError during export: {fnfe}. Ensure the .pt model file exists at the specified path.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during ONNX export: {e}\")\n",
    "        # This can sometimes be due to environment issues or specific model ops not supported by the default ONNX opset.\n",
    "        # Checking the detailed error message from Ultralytics (if any in console) would be helpful.\n",
    "else:\n",
    "    if best_model_pt_path : # If path was formed but file doesn't exist\n",
    "        logging.error(f\"Best .pt model file not found at {best_model_pt_path}. Cannot export to ONNX.\")\n",
    "    # else: PROJECT_DIR/RUN_NAME was missing, already logged.\n",
    "\n",
    "logging.info(\"Cell 13 execution completed: Attempted to export model to ONNX format.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
